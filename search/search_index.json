{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"EasyLLM","text":"<p>EasyLLM is an open source project that provides helpful tools and methods for working with large language models (LLMs), both open source and closed source. </p> <p>EasyLLM implements clients that are compatible with OpenAI's Completion API. This means you can easily replace <code>openai.ChatCompletion</code> with, for example, <code>huggingface.ChatCompletion</code>.</p> <ul> <li>ChatCompletion Clients</li> <li>Prompt Utils</li> <li>Examples</li> </ul>"},{"location":"#getting-started","title":"\ud83d\ude80 Getting Started","text":"<p>Install EasyLLM via pip:</p> <pre><code>pip install easyllm\n</code></pre> <p>Then import and start using the clients:</p> <p><pre><code>from easyllm.clients import huggingface\n\n# helper to build llama2 prompt\nhuggingface.prompt_builder = \"llama2\"\n\nresponse = huggingface.ChatCompletion.create(\n    model=\"meta-llama/Llama-2-70b-chat-hf\",\n    messages=[\n        {\"role\": \"system\", \"content\": \"\\nYou are a helpful assistant speaking like a pirate. argh!\"},\n        {\"role\": \"user\", \"content\": \"What is the sun?\"},\n    ],\n    temperature=0.9,\n    top_p=0.6,\n    max_tokens=256,\n)\n\nprint(response)\n</code></pre> the result will look like </p> <pre><code>{\n  \"id\": \"hf-lVC2iTMkFJ\",\n  \"object\": \"chat.completion\",\n  \"created\": 1690661144,\n  \"model\": \"meta-llama/Llama-2-70b-chat-hf\",\n  \"choices\": [\n    {\n      \"index\": 0,\n      \"message\": {\n        \"role\": \"assistant\",\n        \"content\": \" Arrrr, the sun be a big ol' ball o' fire in the sky, me hearty! It be the source o' light and warmth for our fair planet, and it be a mighty powerful force, savvy? Without the sun, we'd be sailin' through the darkness, lost and cold, so let's give a hearty \\\"Yarrr!\\\" for the sun, me hearties! Arrrr!\"\n      },\n      \"finish_reason\": null\n    }\n  ],\n  \"usage\": {\n    \"prompt_tokens\": 111,\n    \"completion_tokens\": 299,\n    \"total_tokens\": 410\n  }\n}\n</code></pre> <p>Check out other examples:</p> <ul> <li>Detailed ChatCompletion Example</li> <li>Example how to stream chat requests</li> <li>Example how to stream text requests</li> <li>Detailed Completion Example</li> <li>Create Embeddings</li> </ul>"},{"location":"#migration-from-openai-to-huggingface","title":"\ud83d\udcaa\ud83c\udffb Migration from OpenAI to HuggingFace","text":"<p>Migrating from OpenAI to HuggingFace is easy. Just change the import statement and the client you want to use and optionally the prompt builder.</p> <pre><code>- import openai\n+ from easyllm.clients import huggingface\n+ huggingface.prompt_builder = \"llama2\"\n\n\n- response = openai.ChatCompletion.create(\n+ response = huggingface.ChatCompletion.create(\n-    model=\"gpt-3.5-turbo\",\n+    model=\"meta-llama/Llama-2-70b-chat-hf\",\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"Knock knock.\"},\n    ],\n)\n</code></pre> <p>Make sure when you switch your client that your hyperparameters are still valid. For example, <code>temperature</code> of GPT-3 might be different than <code>temperature</code> of <code>Llama-2</code>.</p>"},{"location":"#key-features","title":"\u2611\ufe0f Key Features","text":""},{"location":"#compatible-clients","title":"\ud83e\udd1d Compatible Clients","text":"<ul> <li>Implementation of clients compatible with OpenAI API format of <code>openai.ChatCompletion</code>.</li> <li>Easily switch between different LLMs like <code>openai.ChatCompletion</code> and <code>huggingface.ChatCompletion</code> by changing one line of code. </li> <li>Support for streaming of completions, checkout example How to stream completions.</li> </ul>"},{"location":"#helper-modules","title":"\u2699\ufe0f Helper Modules \u2699\ufe0f","text":"<ul> <li> <p><code>evol_instruct</code> (work in progress) - Use evolutionary algorithms create instructions for LLMs.</p> </li> <li> <p><code>prompt_utils</code> - Helper methods to easily convert between prompt formats like OpenAI Messages to prompts for open source models like Llama 2.</p> </li> </ul>"},{"location":"#citation-acknowledgements","title":"\ud83d\udcd4 Citation &amp; Acknowledgements","text":"<p>If you use EasyLLM, please share it with me on social media or email. I would love to hear about it! You can also cite the project using the following BibTeX:</p> <pre><code>@software{Philipp_Schmid_EasyLLM_2023,\nauthor = {Philipp Schmid},\nlicense = {Apache-2.0},\nmonth = juj,\ntitle = {EasyLLM: Streamlined Tools for LLMs},\nurl = {https://github.com/philschmid/easyllm},\nyear = {2023}\n}\n</code></pre>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#with-pip-recommended","title":"with pip recommended","text":"<p>EasyLLM is published as a [Python package] and can be installed with <code>pip</code> from pypi or from the Github repository, Open up a terminal and install.</p> LatestGithub <pre><code>pip install easyllm\n</code></pre> <pre><code>pip install git+https://github.com/philschmid/easyllm\n</code></pre>"},{"location":"prompt_utils/","title":"Prompt utilities","text":"<p>The <code>prompt_utils</code>  module contains functions to assist with converting Message's Dictionaries into prompts that can be used with <code>ChatCompletion</code> clients. </p> <p>Supported prompt formats:</p> <ul> <li>Prompt utilities</li> <li>Set prompt builder for client</li> <li>Llama 2 Chat builder</li> <li>Vicuna Chat builder</li> <li>Hugging Face ChatML builder<ul> <li>StarChat</li> <li>Falcon</li> </ul> </li> <li>WizardLM Chat builder</li> <li>StableBeluga2 Chat builder</li> <li>Open Assistant Chat builder</li> <li>Anthropic Claude Chat builder</li> </ul> <p>Prompt utils are also exporting a mapping dictionary <code>PROMPT_MAPPING</code> that maps a model name to a prompt builder function. This can be used to select the correct prompt builder function via an environment variable. </p> <pre><code>PROMPT_MAPPING = {\n    \"chatml_falcon\": build_chatml_falcon_prompt,\n    \"chatml_starchat\": build_chatml_starchat_prompt,\n    \"llama2\": build_llama2_prompt,\n    \"open_assistant\": build_open_assistant_prompt,\n    \"stablebeluga\": build_stablebeluga_prompt,\n    \"vicuna\": build_vicuna_prompt,\n    \"wizardlm\": build_wizardlm_prompt,\n}\n</code></pre>"},{"location":"prompt_utils/#set-prompt-builder-for-client","title":"Set prompt builder for client","text":"<pre><code>from easyllm.clients import huggingface\n\nhuggingface.prompt_builder = \"llama2\" # vicuna, chatml_falcon, chatml_starchat, wizardlm, stablebeluga, open_assistant\n</code></pre>"},{"location":"prompt_utils/#llama-2-chat-builder","title":"Llama 2 Chat builder","text":"<p>Creates LLama 2 chat prompt for chat conversations. Learn more in the Hugging Face Blog on how to prompt Llama 2. If a <code>Message</code> with an unsupported <code>role</code> is passed, an error will be thrown.</p> <p>Example Models: </p> <ul> <li>meta-llama/Llama-2-70b-chat-hf</li> </ul> <pre><code>from easyllm.prompt_utils import build_llama2_prompt\n\nmessages=[\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": \"Explain asynchronous programming in the style of the pirate Blackbeard.\"},\n]\nprompt = build_llama2_prompt(messages)\n</code></pre>"},{"location":"prompt_utils/#vicuna-chat-builder","title":"Vicuna Chat builder","text":"<p>Creats a Vicuna prompt for a chat conversation. If a <code>Message</code> with an unsupported <code>role</code> is passed, an error will be thrown. Reference</p> <p>Example Models: </p> <ul> <li>ehartford/WizardLM-13B-V1.0-Uncensored</li> </ul> <pre><code>from easyllm.prompt_utils import build_vicuna_prompt\n\nmessages=[\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": \"Explain asynchronous programming in the style of the pirate Blackbeard.\"},\n]\nprompt = build_vicuna_prompt(messages)\n</code></pre>"},{"location":"prompt_utils/#hugging-face-chatml-builder","title":"Hugging Face ChatML builder","text":"<p>Creates a Hugging Face ChatML prompt for a chat conversation. The Hugging Face ChatML has different prompts for different Example Models, e.g. StarChat or Falcon. If a <code>Message</code> with an unsupported <code>role</code> is passed, an error will be thrown. Reference</p> <p>Example Models:  * HuggingFaceH4/starchat-beta</p>"},{"location":"prompt_utils/#starchat","title":"StarChat","text":"<pre><code>from easyllm.prompt_utils import build_chatml_starchat_prompt\n\nmessages=[\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": \"Explain asynchronous programming in the style of the pirate Blackbeard.\"},\n]\nprompt = build_chatml_starchat_prompt(messages)\n</code></pre>"},{"location":"prompt_utils/#falcon","title":"Falcon","text":"<pre><code>from easyllm.prompt_utils import build_chatml_falcon_prompt\n\nmessages=[\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": \"Explain asynchronous programming in the style of the pirate Blackbeard.\"},\n]\nprompt = build_chatml_falcon_prompt(messages)\n</code></pre>"},{"location":"prompt_utils/#wizardlm-chat-builder","title":"WizardLM Chat builder","text":"<p>Creates a WizardLM prompt for a chat conversation. If a <code>Message</code> with an unsupported <code>role</code> is passed, an error will be thrown. Reference</p> <p>Example Models:</p> <ul> <li>WizardLM/WizardLM-13B-V1.2</li> </ul> <pre><code>from easyllm.prompt_utils import build_wizardlm_prompt\n\nmessages=[\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": \"Explain asynchronous programming in the style of the pirate Blackbeard.\"},\n]\nprompt = build_wizardlm_prompt(messages)\n</code></pre>"},{"location":"prompt_utils/#stablebeluga2-chat-builder","title":"StableBeluga2 Chat builder","text":"<p>Creates StableBeluga2 prompt for a chat conversation. If a <code>Message</code> with an unsupported <code>role</code> is passed, an error will be thrown. Reference</p> <pre><code>from easyllm.prompt_utils import build_stablebeluga_prompt\n\nmessages=[\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": \"Explain asynchronous programming in the style of the pirate Blackbeard.\"},\n]\nprompt = build_stablebeluga_prompt(messages)\n</code></pre>"},{"location":"prompt_utils/#open-assistant-chat-builder","title":"Open Assistant Chat builder","text":"<p>Creates Open Assistant ChatML template. Uses <code>&lt;|prompter|&gt;</code>, <code>&lt;/s&gt;</code>, <code>&lt;|system|&gt;</code>, and <code>&lt;|assistant&gt;</code> tokens. If a . If a <code>Message</code> with an unsupported <code>role</code> is passed, an error will be thrown. Reference</p> <p>Example Models:</p> <ul> <li>OpenAssistant/llama2-13b-orca-8k-3319</li> </ul> <pre><code>from easyllm.prompt_utils import build_open_assistant_prompt\n\nmessages=[\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": \"Explain asynchronous programming in the style of the pirate Blackbeard.\"},\n]\nprompt = build_open_assistant_prompt(messages)\n</code></pre>"},{"location":"prompt_utils/#anthropic-claude-chat-builder","title":"Anthropic Claude Chat builder","text":"<p>Creates Anthropic Claude template. Uses <code>\\n\\nHuman:</code>, <code>\\n\\nAssistant:</code>. If a . If a <code>Message</code> with an unsupported <code>role</code> is passed, an error will be thrown. Reference</p> <p>Example Models:</p> <ul> <li>Bedrock</li> </ul> <pre><code>from easyllm.prompt_utils import build_anthropic_prompt\n\nmessages=[\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": \"Explain asynchronous programming in the style of the pirate Blackbeard.\"},\n]\nprompt = build_anthropic_prompt(messages)\n</code></pre>"},{"location":"clients/","title":"Clients","text":"<p>In the context of EasyLLM, a \"client\" refers to the code that interfaces with a particular LLM API, e.g. OpenAI.</p> <p>Currently supported clients are:  </p> <ul> <li><code>ChatCompletion</code> - ChatCompletion clients are used to interface with LLMs that are compatible with the OpenAI ChatCompletion API.</li> <li><code>Completion</code> - Completion clients are used to interface with LLMs that are compatible with the OpenAI Completion API.</li> <li><code>Embedding</code> - Embedding clients are used to interface with LLMs that are compatible with the OpenAI Embedding API.</li> </ul> <p>Currently supported clients are:  </p>"},{"location":"clients/#hugging-face","title":"Hugging Face","text":"<ul> <li>huggingface.ChatCompletion - a client for interfacing with HuggingFace models that are compatible with the OpenAI ChatCompletion API.</li> <li>huggingface.Completion - a client for interfacing with HuggingFace models that are compatible with the OpenAI Completion API.</li> <li>huggingface.Embedding - a client for interfacing with HuggingFace models that are compatible with the OpenAI Embedding API.</li> </ul>"},{"location":"clients/#amazon-sagemaker","title":"Amazon SageMaker","text":"<ul> <li>sagemaker.ChatCompletion - a client for interfacing with Amazon SageMaker models that are compatible with the OpenAI ChatCompletion API.</li> <li>sagemaker.Completion - a client for interfacing with Amazon SageMaker models that are compatible with the OpenAI Completion API.</li> <li>sagemaker.Embedding - a client for interfacing with Amazon SageMaker models that are compatible with the OpenAI Embedding API.</li> </ul>"},{"location":"clients/#amazon-bedrock","title":"Amazon Bedrock","text":"<ul> <li>bedrock.ChatCompletion - a client for interfacing with Amazon Bedrock models that are compatible with the OpenAI ChatCompletion API.</li> </ul>"},{"location":"clients/bedrock/","title":"Amazon Bedrock","text":"<p>EasyLLM provides a client for interfacing with Amazon Bedrock models. </p> <ul> <li><code>bedrock.ChatCompletion</code> - a client for interfacing with Bedrock models that are compatible with the OpenAI ChatCompletion API.</li> <li><code>bedrock.Completion</code> - a client for interfacing with Bedrock models that are compatible with the OpenAI Completion API.</li> <li><code>bedrock.Embedding</code> - a client for interfacing with Bedrock models that are compatible with the OpenAI Embedding API.</li> </ul>"},{"location":"clients/bedrock/#bedrockchatcompletion","title":"<code>bedrock.ChatCompletion</code>","text":"<p>The <code>bedrock.ChatCompletion</code> client is used to interface with Bedrock models running on Text Generation inference that are compatible with the OpenAI ChatCompletion API. Checkout the Examples</p> <pre><code>import os \n# set env for prompt builder\nos.environ[\"BEDROCK_PROMPT\"] = \"anthropic\" # vicuna, wizardlm, stablebeluga, open_assistant\nos.environ[\"AWS_REGION\"] = \"us-east-1\"  # change to your region\n# os.environ[\"AWS_ACCESS_KEY_ID\"] = \"XXX\" # needed if not using boto3 session\n# os.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"XXX\" # needed if not using boto3 session\n\nfrom easyllm.clients import bedrock\n\nresponse = bedrock.ChatCompletion.create(\n    model=\"anthropic.claude-v2\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"What is 2 + 2?\"},\n    ],\n      temperature=0.9,\n      top_p=0.6,\n      max_tokens=1024,\n      debug=False,\n)\n</code></pre> <p>Supported parameters are:</p> <ul> <li><code>model</code> - The model to use for the completion. If not provided, defaults to the base url.</li> <li><code>messages</code> - <code>List[ChatMessage]</code> to use for the completion.</li> <li><code>temperature</code> - The temperature to use for the completion. Defaults to 0.9.</li> <li><code>top_p</code> - The top_p to use for the completion. Defaults to 0.6.</li> <li><code>top_k</code> - The top_k to use for the completion. Defaults to 10.</li> <li><code>n</code> - The number of completions to generate. Defaults to 1.</li> <li><code>max_tokens</code> - The maximum number of tokens to generate. Defaults to 1024.</li> <li><code>stop</code> - The stop sequence(s) to use for the completion. Defaults to None.</li> <li><code>stream</code> - Whether to stream the completion. Defaults to False.</li> <li><code>debug</code> - Whether to enable debug logging. Defaults to False.</li> </ul>"},{"location":"clients/bedrock/#build-prompt","title":"Build Prompt","text":"<p>By default the <code>bedrock</code> client will try to read the <code>BEDROCK_PROMPT</code> environment variable and tries to map the value to the <code>PROMPT_MAPPING</code> dictionary. If this is not set, it will use the default prompt builder.  You can also set it manually.</p> <p>Checkout the Prompt Utils for more details.</p> <p>manually setting the prompt builder:</p> <pre><code>from easyllm.clients import bedrock\n\nbedrock.prompt_builder = \"anthropic\"\n\nres = bedrock.ChatCompletion.create(...)\n</code></pre> <p>Using environment variable:</p> <pre><code># can happen elsehwere\nimport os\nos.environ[\"BEDROCK_PROMPT\"] = \"anthropic\"\n\nfrom easyllm.clients import bedrock\n</code></pre>"},{"location":"clients/huggingface/","title":"Hugging Face","text":"<p>EasyLLM provides a client for interfacing with HuggingFace models. The client is compatible with the HuggingFace Inference API, Hugging Face Inference Endpoints or any Web Service running Text Generation Inference or compatible API endpoints. </p> <ul> <li><code>huggingface.ChatCompletion</code> - a client for interfacing with HuggingFace models that are compatible with the OpenAI ChatCompletion API.</li> <li><code>huggingface.Completion</code> - a client for interfacing with HuggingFace models that are compatible with the OpenAI Completion API.</li> <li><code>huggingface.Embedding</code> - a client for interfacing with HuggingFace models that are compatible with the OpenAI Embedding API.</li> </ul>"},{"location":"clients/huggingface/#huggingfacechatcompletion","title":"<code>huggingface.ChatCompletion</code>","text":"<p>The <code>huggingface.ChatCompletion</code> client is used to interface with HuggingFace models running on Text Generation inference that are compatible with the OpenAI ChatCompletion API. Checkout the Examples for more details and How to stream completions for an example how to stream requests.</p> <pre><code>from easyllm.clients import huggingface\n\n# The module automatically loads the HuggingFace API key from the environment variable HUGGINGFACE_TOKEN or from the HuggingFace CLI configuration file.\n# huggingface.api_key=\"hf_xxx\"\nhuggingface.prompt_builder = \"llama2\"\n\nresponse = huggingface.ChatCompletion.create(\n    model=\"meta-llama/Llama-2-70b-chat-hf\",\n    messages=[\n        {\"role\": \"system\", \"content\": \"\\nYou are a helpful, respectful and honest assistant.\"},\n        {\"role\": \"user\", \"content\": \"Knock knock.\"},\n    ],\n    temperature=0.9,\n    top_p=0.6,\n    max_tokens=1024,\n)\n</code></pre> <p>Supported parameters are:</p> <ul> <li><code>model</code> - The model to use for the completion. If not provided, defaults to the base url.</li> <li><code>messages</code> - <code>List[ChatMessage]</code> to use for the completion.</li> <li><code>temperature</code> - The temperature to use for the completion. Defaults to 0.9.</li> <li><code>top_p</code> - The top_p to use for the completion. Defaults to 0.6.</li> <li><code>top_k</code> - The top_k to use for the completion. Defaults to 10.</li> <li><code>n</code> - The number of completions to generate. Defaults to 1.</li> <li><code>max_tokens</code> - The maximum number of tokens to generate. Defaults to 1024.</li> <li><code>stop</code> - The stop sequence(s) to use for the completion. Defaults to None.</li> <li><code>stream</code> - Whether to stream the completion. Defaults to False.</li> <li><code>frequency_penalty</code> - The frequency penalty to use for the completion. Defaults to 1.0.</li> <li><code>debug</code> - Whether to enable debug logging. Defaults to False.</li> </ul>"},{"location":"clients/huggingface/#huggingfacecompletion","title":"<code>huggingface.Completion</code>","text":"<p>The <code>huggingface.Completion</code> client is used to interface with HuggingFace models running on Text Generation inference that are compatible with the OpenAI Completion API. Checkout the Examples for more details and How to stream completions for an example how to stream requests.</p> <pre><code>from easyllm.clients import huggingface\n\n# The module automatically loads the HuggingFace API key from the environment variable HUGGINGFACE_TOKEN or from the HuggingFace CLI configuration file.\n# huggingface.api_key=\"hf_xxx\"\nhubbingface.prompt_builder = \"llama2\"\n\nresponse = huggingface.Completion.create(\n    model=\"meta-llama/Llama-2-70b-chat-hf\",\n    prompt=\"What is the meaning of life?\",\n    temperature=0.9,\n    top_p=0.6,\n    max_tokens=1024,\n)\n</code></pre> <p>Supported parameters are:</p> <ul> <li><code>model</code> - The model to use for the completion. If not provided, defaults to the base url.</li> <li><code>prompt</code> -  Text to use for the completion, if prompt_builder is set, prompt will be formatted with the prompt_builder.</li> <li><code>temperature</code> - The temperature to use for the completion. Defaults to 0.9.</li> <li><code>top_p</code> - The top_p to use for the completion. Defaults to 0.6.</li> <li><code>top_k</code> - The top_k to use for the completion. Defaults to 10.</li> <li><code>n</code> - The number of completions to generate. Defaults to 1.</li> <li><code>max_tokens</code> - The maximum number of tokens to generate. Defaults to 1024.</li> <li><code>stop</code> - The stop sequence(s) to use for the completion. Defaults to None.</li> <li><code>stream</code> - Whether to stream the completion. Defaults to False.</li> <li><code>frequency_penalty</code> - The frequency penalty to use for the completion. Defaults to 1.0.</li> <li><code>debug</code> - Whether to enable debug logging. Defaults to False.</li> <li><code>echo</code> - Whether to echo the prompt. Defaults to False.</li> <li><code>logprobs</code> - Weather to return logprobs. Defaults to None.</li> </ul>"},{"location":"clients/huggingface/#huggingfaceembedding","title":"<code>huggingface.Embedding</code>","text":"<p>The <code>huggingface.Embedding</code> client is used to interface with HuggingFace models running as an API that are compatible with the OpenAI Embedding API. Checkout the Examples for more details.</p> <pre><code>from easyllm.clients import huggingface\n\n# The module automatically loads the HuggingFace API key from the environment variable HUGGINGFACE_TOKEN or from the HuggingFace CLI configuration file.\n# huggingface.api_key=\"hf_xxx\"\n\nembedding = huggingface.Embedding.create(\n    model=\"sentence-transformers/all-MiniLM-L6-v2\",\n    text=\"What is the meaning of life?\",\n)\n\nlen(embedding[\"data\"][0][\"embedding\"])\n</code></pre> <p>Supported parameters are:</p> <ul> <li><code>model</code> - The model to use to create the embedding. If not provided, defaults to the base url.</li> <li><code>input</code> -  <code>Union[str, List[str]]</code> document(s) to embed.</li> </ul>"},{"location":"clients/huggingface/#environment-configuration","title":"Environment Configuration","text":"<p>You can configure the <code>huggingface</code> client by setting environment variables or overwriting the default values. See below on how to adjust the HF token, url and prompt builder.</p>"},{"location":"clients/huggingface/#setting-hf-token","title":"Setting HF token","text":"<p>By default the <code>huggingface</code> client will try to read the <code>HUGGINGFACE_TOKEN</code> environment variable. If this is not set, it will try to read the token from the <code>~/.huggingface</code> folder. If this is not set, it will not use a token.</p> <p>Alternatively you can set the token manually by setting <code>huggingface.api_key</code>.</p> <p>manually setting the api key:</p> <pre><code>from easyllm.clients import huggingface\n\nhuggingface.api_key=\"hf_xxx\"\n\nres = huggingface.ChatCompletion.create(...)\n</code></pre> <p>Using environment variable:</p> <pre><code># can happen elsehwere\nimport os\nos.environ[\"HUGGINGFACE_TOKEN\"] = \"hf_xxx\"\n\nfrom easyllm.clients import huggingface\n</code></pre>"},{"location":"clients/huggingface/#changing-url","title":"Changing url","text":"<p>By default the <code>huggingface</code> client will try to read the <code>HUGGINGFACE_API_BASE</code> environment variable. If this is not set, it will use the default url <code>https://api-inference.huggingface.co/models</code>. This is helpful if you want to use a different url like <code>https://zj5lt7pmzqzbp0d1.us-east-1.aws.endpoints.huggingface.cloud</code> or a local url like <code>http://localhost:8000</code> or an Hugging Face Inference Endpoint.</p> <p>Alternatively you can set the url manually by setting <code>huggingface.api_base</code>. If you set a custom you have to leave the <code>model</code> parameter empty. </p> <p>manually setting the api base:</p> <pre><code>from easyllm.clients import huggingface\n\nhuggingface.api_base=\"https://my-url\"\n\n\nres = huggingface.ChatCompletion.create(...)\n</code></pre> <p>Using environment variable:</p> <pre><code># can happen elsehwere\nimport os\nos.environ[\"HUGGINGFACE_API_BASE\"] = \"https://my-url\"\n\nfrom easyllm.clients import huggingface\n</code></pre>"},{"location":"clients/huggingface/#build-prompt","title":"Build Prompt","text":"<p>By default the <code>huggingface</code> client will try to read the <code>HUGGINGFACE_PROMPT</code> environment variable and tries to map the value to the <code>PROMPT_MAPPING</code> dictionary. If this is not set, it will use the default prompt builder.  You can also set it manually.</p> <p>Checkout the Prompt Utils for more details.</p> <p>manually setting the prompt builder:</p> <pre><code>from easyllm.clients import huggingface\n\nhuggingface.prompt_builder = \"llama2\"\n\nres = huggingface.ChatCompletion.create(...)\n</code></pre> <p>Using environment variable:</p> <pre><code># can happen elsehwere\nimport os\nos.environ[\"HUGGINGFACE_PROMPT\"] = \"llama2\"\n\nfrom easyllm.clients import huggingface\n</code></pre>"},{"location":"clients/sagemaker/","title":"Amazon SageMaker","text":"<p>EasyLLM provides a client for interfacing with Amazon SageMaker models. </p> <ul> <li><code>sagemaker.ChatCompletion</code> - a client for interfacing with sagemaker models that are compatible with the OpenAI ChatCompletion API.</li> <li><code>sagemaker.Completion</code> - a client for interfacing with sagemaker models that are compatible with the OpenAI Completion API.</li> <li><code>sagemaker.Embedding</code> - a client for interfacing with sagemaker models that are compatible with the OpenAI Embedding API.</li> </ul>"},{"location":"clients/sagemaker/#sagemakerchatcompletion","title":"<code>sagemaker.ChatCompletion</code>","text":"<p>The <code>sagemaker.ChatCompletion</code> client is used to interface with sagemaker models running on Text Generation inference that are compatible with the OpenAI ChatCompletion API. Checkout the Examples</p> <pre><code>import os \nfrom easyllm.clients import sagemaker\n\n# set env for prompt builder\nos.environ[\"HUGGINGFACE_PROMPT\"] = \"llama2\" # vicuna, wizardlm, stablebeluga, open_assistant\nos.environ[\"AWS_REGION\"] = \"us-east-1\"  # change to your region\n# os.environ[\"AWS_ACCESS_KEY_ID\"] = \"XXX\" # needed if not using boto3 session\n# os.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"XXX\" # needed if not using boto3 session\n\n\nresponse = sagemaker.ChatCompletion.create(\n    model=\"huggingface-pytorch-tgi-inference-2023-08-08-14-15-52-703\",\n    messages=[\n        {\"role\": \"system\", \"content\": \"\\nYou are a helpful, respectful and honest assistant.\"},\n        {\"role\": \"user\", \"content\": \"Knock knock.\"},\n    ],\n    temperature=0.9,\n    top_p=0.6,\n    max_tokens=1024,\n)\n</code></pre> <p>Supported parameters are:</p> <ul> <li><code>model</code> - The model to use for the completion. If not provided, defaults to the base url.</li> <li><code>messages</code> - <code>List[ChatMessage]</code> to use for the completion.</li> <li><code>temperature</code> - The temperature to use for the completion. Defaults to 0.9.</li> <li><code>top_p</code> - The top_p to use for the completion. Defaults to 0.6.</li> <li><code>top_k</code> - The top_k to use for the completion. Defaults to 10.</li> <li><code>n</code> - The number of completions to generate. Defaults to 1.</li> <li><code>max_tokens</code> - The maximum number of tokens to generate. Defaults to 1024.</li> <li><code>stop</code> - The stop sequence(s) to use for the completion. Defaults to None.</li> <li><code>stream</code> - Whether to stream the completion. Defaults to False.</li> <li><code>frequency_penalty</code> - The frequency penalty to use for the completion. Defaults to 1.0.</li> <li><code>debug</code> - Whether to enable debug logging. Defaults to False.</li> </ul>"},{"location":"clients/sagemaker/#sagemakercompletion","title":"<code>sagemaker.Completion</code>","text":"<p>The <code>sagemaker.Completion</code> client is used to interface with sagemaker models running on Text Generation inference that are compatible with the OpenAI Completion API. Checkout the Examples.</p> <pre><code>import os \nfrom easyllm.clients import sagemaker\n\n# set env for prompt builder\nos.environ[\"HUGGINGFACE_PROMPT\"] = \"llama2\" # vicuna, wizardlm, stablebeluga, open_assistant\nos.environ[\"AWS_REGION\"] = \"us-east-1\"  # change to your region\n# os.environ[\"AWS_ACCESS_KEY_ID\"] = \"XXX\" # needed if not using boto3 session\n# os.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"XXX\" # needed if not using boto3 session\n\nresponse = sagemaker.Completion.create(\n    model=\"meta-llama/Llama-2-70b-chat-hf\",\n    prompt=\"What is the meaning of life?\",\n    temperature=0.9,\n    top_p=0.6,\n    max_tokens=1024,\n)\n</code></pre> <p>Supported parameters are:</p> <ul> <li><code>model</code> - The model to use for the completion. If not provided, defaults to the base url.</li> <li><code>prompt</code> -  Text to use for the completion, if prompt_builder is set, prompt will be formatted with the prompt_builder.</li> <li><code>temperature</code> - The temperature to use for the completion. Defaults to 0.9.</li> <li><code>top_p</code> - The top_p to use for the completion. Defaults to 0.6.</li> <li><code>top_k</code> - The top_k to use for the completion. Defaults to 10.</li> <li><code>n</code> - The number of completions to generate. Defaults to 1.</li> <li><code>max_tokens</code> - The maximum number of tokens to generate. Defaults to 1024.</li> <li><code>stop</code> - The stop sequence(s) to use for the completion. Defaults to None.</li> <li><code>stream</code> - Whether to stream the completion. Defaults to False.</li> <li><code>frequency_penalty</code> - The frequency penalty to use for the completion. Defaults to 1.0.</li> <li><code>debug</code> - Whether to enable debug logging. Defaults to False.</li> <li><code>echo</code> - Whether to echo the prompt. Defaults to False.</li> <li><code>logprobs</code> - Weather to return logprobs. Defaults to None.</li> </ul>"},{"location":"clients/sagemaker/#sagemakerembedding","title":"<code>sagemaker.Embedding</code>","text":"<p>The <code>sagemaker.Embedding</code> client is used to interface with sagemaker models running as an API that are compatible with the OpenAI Embedding API. Checkout the Examples for more details.</p> <pre><code>import os \n# set env for prompt builder\nos.environ[\"HUGGINGFACE_PROMPT\"] = \"llama2\" # vicuna, wizardlm, stablebeluga, open_assistant\nos.environ[\"AWS_REGION\"] = \"us-east-1\"  # change to your region\n# os.environ[\"AWS_ACCESS_KEY_ID\"] = \"XXX\" # needed if not using boto3 session\n# os.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"XXX\" # needed if not using boto3 session\n\nfrom easyllm.clients import sagemaker\n\nembedding = sagemaker.Embedding.create(\n    model=\"SageMakerModelEmbeddingEndpoint24E49D09-64prhjuiWUtE\",\n    input=\"That's a nice car.\",\n)\n\nlen(embedding[\"data\"][0][\"embedding\"])\n</code></pre> <p>Supported parameters are:</p> <ul> <li><code>model</code> - The model to use to create the embedding. If not provided, defaults to the base url.</li> <li><code>input</code> -  <code>Union[str, List[str]]</code> document(s) to embed.</li> </ul>"},{"location":"clients/sagemaker/#environment-configuration","title":"Environment Configuration","text":"<p>You can configure the <code>sagemaker</code> client by setting environment variables or overwriting the default values. See below on how to adjust the HF token, url and prompt builder.</p>"},{"location":"clients/sagemaker/#setting-credentials","title":"Setting Credentials","text":"<p>By default the <code>sagemaker</code> client will try to read the <code>AWS_ACCESS_KEY_ID</code> and <code>AWS_SECRET_ACCESS_KEY</code> environment variable. If this is not set, it will try to use <code>boto3</code>. </p> <p>Alternatively you can set the token manually by setting <code>sagemaker.*</code>.</p> <p>manually setting the api key:</p> <pre><code>from easyllm.clients import sagemaker\n\nsagemaker.api_aws_access_key=\"xxx\"\nsagemaker.api_aws_secret_key=\"xxx\"\n\nres = sagemaker.ChatCompletion.create(...)\n</code></pre> <p>Using environment variable:</p> <pre><code># can happen elsehwere\nimport os\nos.environ[\"AWS_ACCESS_KEY_ID\"] = \"xxx\"\nos.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"xxx\"\n\nfrom easyllm.clients import sagemaker\n</code></pre>"},{"location":"clients/sagemaker/#build-prompt","title":"Build Prompt","text":"<p>By default the <code>sagemaker</code> client will try to read the <code>sagemaker_PROMPT</code> environment variable and tries to map the value to the <code>PROMPT_MAPPING</code> dictionary. If this is not set, it will use the default prompt builder.  You can also set it manually.</p> <p>Checkout the Prompt Utils for more details.</p> <p>manually setting the prompt builder:</p> <pre><code>from easyllm.clients import sagemaker\n\nsagemaker.prompt_builder = \"llama2\"\n\nres = sagemaker.ChatCompletion.create(...)\n</code></pre> <p>Using environment variable:</p> <pre><code># can happen elsehwere\nimport os\nos.environ[\"HUGGINGFACE_PROMPT\"] = \"llama2\"\n\nfrom easyllm.clients import sagemaker\n</code></pre>"},{"location":"examples/","title":"Examples","text":"<p>Here are some examples to help you get started with the easyllm library:</p>"},{"location":"examples/#hugging-face","title":"Hugging Face","text":"Example Description Detailed ChatCompletion Example Shows how to use the ChatCompletion API to have a conversational chat with the model. Detailed Completion Example Uses the TextCompletion API to generate text with the model. Create Embeddings Embeds text into vector representations using the model. Example how to stream chat requests Demonstrates streaming multiple chat requests to efficiently chat with the model. Example how to stream text requests Shows how to stream multiple text completion requests. Hugging Face Inference Endpoints Example Example on how to use custom endpoints, e.g. Inference Endpoints or localhost. Retrieval Augmented Generation using Llama 2 Example on how to use Llama 2 70B for in-context retrival augmentation Llama 2 70B Agent/Tool use example  Example on how to use Llama 2 70B to interace with tools and could be used as an agent <p>The examples cover the main functionality of the library - chat, text completion, and embeddings. Let me know if you would like me to modify or expand the index page in any way.</p>"},{"location":"examples/#amazon-sagemaker","title":"Amazon SageMaker","text":"Example Description Detailed ChatCompletion Example Shows how to use the ChatCompletion API to have a conversational chat with the model. Detailed Completion Example Uses the TextCompletion API to generate text with the model. Create Embeddings Embeds text into vector representations using the model."},{"location":"examples/#amazon-bedrock","title":"Amazon Bedrock","text":"Example Description Detailed ChatCompletion Example Shows how to use the ChatCompletion API to have a conversational chat with the model. Example how to stream chat requests Demonstrates streaming multiple chat requests to efficiently chat with the model."},{"location":"examples/bedrock-chat-completion-api/","title":"How to use Chat Completion clients with Amazon Bedrock","text":"In\u00a0[\u00a0]: Copied! <pre># if needed, install and/or upgrade to the latest version of the EasyLLM Python library\n%pip install --upgrade easyllm[bedrock]\n</pre> # if needed, install and/or upgrade to the latest version of the EasyLLM Python library %pip install --upgrade easyllm[bedrock]  In\u00a0[6]: Copied! <pre># import the EasyLLM Python library for calling the EasyLLM API\nimport easyllm\n</pre> # import the EasyLLM Python library for calling the EasyLLM API import easyllm In\u00a0[2]: Copied! <pre>import os \n# set env for prompt builder\nos.environ[\"BEDROCK_PROMPT\"] = \"anthropic\" # vicuna, wizardlm, stablebeluga, open_assistant\nos.environ[\"AWS_REGION\"] = \"us-east-1\"  # change to your region\n# os.environ[\"AWS_ACCESS_KEY_ID\"] = \"XXX\" # needed if not using boto3 session\n# os.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"XXX\" # needed if not using boto3 session\n\nfrom easyllm.clients import bedrock\n\nresponse = bedrock.ChatCompletion.create(\n    model=\"anthropic.claude-v2\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"What is 2 + 2?\"},\n    ],\n      temperature=0.9,\n      top_p=0.6,\n      max_tokens=1024,\n      debug=False,\n)\nresponse\n</pre> import os  # set env for prompt builder os.environ[\"BEDROCK_PROMPT\"] = \"anthropic\" # vicuna, wizardlm, stablebeluga, open_assistant os.environ[\"AWS_REGION\"] = \"us-east-1\"  # change to your region # os.environ[\"AWS_ACCESS_KEY_ID\"] = \"XXX\" # needed if not using boto3 session # os.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"XXX\" # needed if not using boto3 session  from easyllm.clients import bedrock  response = bedrock.ChatCompletion.create(     model=\"anthropic.claude-v2\",     messages=[         {\"role\": \"user\", \"content\": \"What is 2 + 2?\"},     ],       temperature=0.9,       top_p=0.6,       max_tokens=1024,       debug=False, ) response   <pre>{'completion': ' 2 + 2 = 4', 'stop_reason': 'stop_sequence'}\n</pre> Out[2]: <pre>{'id': 'hf-Mf7UqliZQP',\n 'object': 'chat.completion',\n 'created': 1698333425,\n 'model': 'anthropic.claude-v2',\n 'choices': [{'index': 0,\n   'message': {'role': 'assistant', 'content': '2 + 2 = 4'},\n   'finish_reason': 'stop_sequence'}],\n 'usage': {'prompt_tokens': 9, 'completion_tokens': 9, 'total_tokens': 18}}</pre> <p>As you can see, the response object has a few fields:</p> <ul> <li><code>id</code>: the ID of the request</li> <li><code>object</code>: the type of object returned (e.g., <code>chat.completion</code>)</li> <li><code>created</code>: the timestamp of the request</li> <li><code>model</code>: the full name of the model used to generate the response</li> <li><code>usage</code>: the number of tokens used to generate the replies, counting prompt, completion, and total</li> <li><code>choices</code>: a list of completion objects (only one, unless you set <code>n</code> greater than 1)<ul> <li><code>message</code>: the message object generated by the model, with <code>role</code> and <code>content</code></li> <li><code>finish_reason</code>: the reason the model stopped generating text (either <code>stop</code>, or <code>length</code> if <code>max_tokens</code> limit was reached)</li> <li><code>index</code>: the index of the completion in the list of choices</li> </ul> </li> </ul> <p>Extract just the reply with:</p> In\u00a0[3]: Copied! <pre>print(response['choices'][0]['message']['content'])\n</pre> print(response['choices'][0]['message']['content']) <pre>2 + 2 = 4\n</pre> <p>Even non-conversation-based tasks can fit into the chat format, by placing the instruction in the first user message.</p> <p>For example, to ask the model to explain asynchronous programming in the style of the pirate Blackbeard, we can structure conversation as follows:</p> In\u00a0[4]: Copied! <pre># example with a system message\nresponse = bedrock.ChatCompletion.create(\n    model=\"anthropic.claude-v2\",\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"Explain asynchronous programming in the style of math teacher.\"},\n    ],\n)\n\nprint(response['choices'][0]['message']['content'])\n</pre> # example with a system message response = bedrock.ChatCompletion.create(     model=\"anthropic.claude-v2\",     messages=[         {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},         {\"role\": \"user\", \"content\": \"Explain asynchronous programming in the style of math teacher.\"},     ], )  print(response['choices'][0]['message']['content'])  <pre>{'completion': ' Okay class, today we\\'re going to learn about asynchronous programming. Asynchronous means things happening at different times, not necessarily in order. It\\'s like when you\\'re cooking dinner - you might put the pasta on to boil, then start chopping vegetables while the pasta cooks. You don\\'t have to wait for the pasta to finish boiling before you can start on the vegetables. The two tasks are happening asynchronously. \\n\\nIn programming, asynchronous functions allow the code to execute other operations while waiting for a long-running task to complete. Let\\'s look at an example:\\n\\n```js\\nfunction cookPasta() {\\n  console.log(\"Putting pasta on to boil...\");\\n  // Simulate a long task\\n  setTimeout(() =&gt; {\\n    console.log(\"Pasta done!\");\\n  }, 5000); \\n}\\n\\nfunction chopVegetables() {\\n  console.log(\"Chopping vegetables...\");\\n}\\n\\ncookPasta();\\nchopVegetables();\\n```\\n\\nWhen we call `cookPasta()`, it starts the timer but doesn\\'t wait 5 seconds - it immediately moves on to calling `chopVegetables()`. So the two functions run asynchronously. \\n\\nThe key is that `cookPasta()` is non-blocking - it doesn\\'t stop the rest of the code from running while it completes. This allows us to maximize efficiency and not waste time waiting.\\n\\nSo in summary, asynchronous programming allows multiple operations to happen independently of each other, like cooking a meal. We avoid blocking code execution by using asynchronous functions. Any questions on this?', 'stop_reason': 'stop_sequence'}\nOkay class, today we're going to learn about asynchronous programming. Asynchronous means things happening at different times, not necessarily in order. It's like when you're cooking dinner - you might put the pasta on to boil, then start chopping vegetables while the pasta cooks. You don't have to wait for the pasta to finish boiling before you can start on the vegetables. The two tasks are happening asynchronously. \n\nIn programming, asynchronous functions allow the code to execute other operations while waiting for a long-running task to complete. Let's look at an example:\n\n```js\nfunction cookPasta() {\n  console.log(\"Putting pasta on to boil...\");\n  // Simulate a long task\n  setTimeout(() =&gt; {\n    console.log(\"Pasta done!\");\n  }, 5000); \n}\n\nfunction chopVegetables() {\n  console.log(\"Chopping vegetables...\");\n}\n\ncookPasta();\nchopVegetables();\n```\n\nWhen we call `cookPasta()`, it starts the timer but doesn't wait 5 seconds - it immediately moves on to calling `chopVegetables()`. So the two functions run asynchronously. \n\nThe key is that `cookPasta()` is non-blocking - it doesn't stop the rest of the code from running while it completes. This allows us to maximize efficiency and not waste time waiting.\n\nSo in summary, asynchronous programming allows multiple operations to happen independently of each other, like cooking a meal. We avoid blocking code execution by using asynchronous functions. Any questions on this?\n</pre> In\u00a0[5]: Copied! <pre># example without a system message and debug flag on:\nresponse = bedrock.ChatCompletion.create(\n    model=\"anthropic.claude-v2\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"Explain asynchronous programming in the style of the pirate Blackbeard.\"},\n    ]\n)\n\nprint(response['choices'][0]['message']['content'])\n</pre> # example without a system message and debug flag on: response = bedrock.ChatCompletion.create(     model=\"anthropic.claude-v2\",     messages=[         {\"role\": \"user\", \"content\": \"Explain asynchronous programming in the style of the pirate Blackbeard.\"},     ] )  print(response['choices'][0]['message']['content'])  <pre>{'completion': \" Aye matey! Asynchronous programming be when ye fire yer cannons without waiting fer each shot to hit. Ye keep loadin' and shootin' while the cannonballs sail through the air. Ye don't know exactly when they'll strike the target, but ye keep sendin' 'em off. \\n\\nThe ship keeps movin' forward, not stalled waiting fer each blast. Other pirates keep swabbin' the decks and hoistin' the sails so we make progress while the cannons thunder. We tie callbacks to the cannons to handle the boom when they finally hit.\\n\\nArrr! Asynchronous programmin' means ye do lots o' tasks at once, not blocked by waitin' fer each one to finish. Ye move ahead and let functions handle the results when ready. It be faster than linear code that stops at each step. Thar be treasures ahead, lads! Keep those cannons roarin'!\", 'stop_reason': 'stop_sequence'}\nAye matey! Asynchronous programming be when ye fire yer cannons without waiting fer each shot to hit. Ye keep loadin' and shootin' while the cannonballs sail through the air. Ye don't know exactly when they'll strike the target, but ye keep sendin' 'em off. \n\nThe ship keeps movin' forward, not stalled waiting fer each blast. Other pirates keep swabbin' the decks and hoistin' the sails so we make progress while the cannons thunder. We tie callbacks to the cannons to handle the boom when they finally hit.\n\nArrr! Asynchronous programmin' means ye do lots o' tasks at once, not blocked by waitin' fer each one to finish. Ye move ahead and let functions handle the results when ready. It be faster than linear code that stops at each step. Thar be treasures ahead, lads! Keep those cannons roarin'!\n</pre> In\u00a0[6]: Copied! <pre># An example of a faked few-shot conversation to prime the model into translating business jargon to simpler speech\nresponse = bedrock.ChatCompletion.create(\n    model=\"anthropic.claude-v2\",\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful, pattern-following assistant.\"},\n        {\"role\": \"user\", \"content\": \"Help me translate the following corporate jargon into plain English.\"},\n        {\"role\": \"assistant\", \"content\": \"Sure, I'd be happy to!\"},\n        {\"role\": \"user\", \"content\": \"New synergies will help drive top-line growth.\"},\n        {\"role\": \"assistant\", \"content\": \"Things working well together will increase revenue.\"},\n        {\"role\": \"user\", \"content\": \"Let's circle back when we have more bandwidth to touch base on opportunities for increased leverage.\"},\n        {\"role\": \"assistant\", \"content\": \"Let's talk later when we're less busy about how to do better.\"},\n        {\"role\": \"user\", \"content\": \"This late pivot means we don't have time to boil the ocean for the client deliverable.\"},\n    ],\n)\n\nprint(response[\"choices\"][0][\"message\"][\"content\"])\n</pre> # An example of a faked few-shot conversation to prime the model into translating business jargon to simpler speech response = bedrock.ChatCompletion.create(     model=\"anthropic.claude-v2\",     messages=[         {\"role\": \"system\", \"content\": \"You are a helpful, pattern-following assistant.\"},         {\"role\": \"user\", \"content\": \"Help me translate the following corporate jargon into plain English.\"},         {\"role\": \"assistant\", \"content\": \"Sure, I'd be happy to!\"},         {\"role\": \"user\", \"content\": \"New synergies will help drive top-line growth.\"},         {\"role\": \"assistant\", \"content\": \"Things working well together will increase revenue.\"},         {\"role\": \"user\", \"content\": \"Let's circle back when we have more bandwidth to touch base on opportunities for increased leverage.\"},         {\"role\": \"assistant\", \"content\": \"Let's talk later when we're less busy about how to do better.\"},         {\"role\": \"user\", \"content\": \"This late pivot means we don't have time to boil the ocean for the client deliverable.\"},     ], )  print(response[\"choices\"][0][\"message\"][\"content\"])  <pre>{'completion': \" Changing direction at the last minute means we don't have time to do an exhaustive analysis for what we're providing to the client.\", 'stop_reason': 'stop_sequence'}\nChanging direction at the last minute means we don't have time to do an exhaustive analysis for what we're providing to the client.\n</pre> <p>Not every attempt at engineering conversations will succeed at first.</p> <p>If your first attempts fail, don't be afraid to experiment with different ways of priming or conditioning the model.</p> <p>As an example, one developer discovered an increase in accuracy when they inserted a user message that said \"Great job so far, these have been perfect\" to help condition the model into providing higher quality responses.</p> <p>For more ideas on how to lift the reliability of the models, consider reading our guide on techniques to increase reliability. It was written for non-chat models, but many of its principles still apply.</p>"},{"location":"examples/bedrock-chat-completion-api/#how-to-use-chat-completion-clients-with-amazon-bedrock","title":"How to use Chat Completion clients with Amazon Bedrock\u00b6","text":"<p>EasyLLM can be used as an abstract layer to replace <code>gpt-3.5-turbo</code> and <code>gpt-4</code> with Amazon Bedrock models.</p> <p>You can change your own applications from the OpenAI API, by simply changing the client.</p> <p>Chat models take a series of messages as input, and return an AI-written message as output.</p> <p>This guide illustrates the chat format with a few example API calls.</p>"},{"location":"examples/bedrock-chat-completion-api/#0-setup","title":"0. Setup\u00b6","text":"<p>Before you can use <code>easyllm</code> with Amazon Bedrock you need setup permission and access to the models. You can do this by following of the instructions below:</p> <ul> <li>https://docs.aws.amazon.com/IAM/latest/UserGuide/getting-set-up.html</li> <li>https://docs.aws.amazon.com/IAM/latest/UserGuide/troubleshoot_access-denied.html</li> <li>https://docs.aws.amazon.com/bedrock/latest/userguide/security-iam.html</li> </ul>"},{"location":"examples/bedrock-chat-completion-api/#1-import-the-easyllm-library","title":"1. Import the easyllm library\u00b6","text":""},{"location":"examples/bedrock-chat-completion-api/#2-an-example-chat-api-call","title":"2. An example chat API call\u00b6","text":"<p>A chat API call has two required inputs:</p> <ul> <li><code>model</code>: the name of the model you want to use (e.g., <code>huggingface-pytorch-tgi-inference-2023-08-08-14-15-52-703</code>) or leave it empty to just call the api</li> <li><code>messages</code>: a list of message objects, where each object has two required fields:<ul> <li><code>role</code>: the role of the messenger (either <code>system</code>, <code>user</code>, or <code>assistant</code>)</li> <li><code>content</code>: the content of the message (e.g., <code>Write me a beautiful poem</code>)</li> </ul> </li> </ul> <p>Compared to OpenAI api is the <code>huggingface</code> module also exposing a <code>prompt_builder</code> and <code>stop_sequences</code> parameter you can use to customize the prompt and stop sequences. The EasyLLM package comes with prompt builder utilities.</p> <p>Let's look at an example chat API calls to see how the chat format works in practice.</p>"},{"location":"examples/bedrock-chat-completion-api/#3-few-shot-prompting","title":"3. Few-shot prompting\u00b6","text":"<p>In some cases, it's easier to show the model what you want rather than tell the model what you want.</p> <p>One way to show the model what you want is with faked example messages.</p> <p>For example:</p>"},{"location":"examples/bedrock-stream-chat-completions/","title":"How to stream Chat Completion requests with Amazon Bedrock","text":"In\u00a0[\u00a0]: Copied! <pre># if needed, install and/or upgrade to the latest version of the EasyLLM Python library\n%pip install --upgrade easyllm[bedrock]\n</pre> # if needed, install and/or upgrade to the latest version of the EasyLLM Python library %pip install --upgrade easyllm[bedrock]  In\u00a0[1]: Copied! <pre># imports\nimport easyllm  # for API calls\n</pre> # imports import easyllm  # for API calls In\u00a0[1]: Copied! <pre>import os \n# set env for prompt builder\nos.environ[\"BEDROCK_PROMPT\"] = \"anthropic\" # vicuna, wizardlm, stablebeluga, open_assistant\nos.environ[\"AWS_REGION\"] = \"us-east-1\"  # change to your region\n# os.environ[\"AWS_ACCESS_KEY_ID\"] = \"XXX\" # needed if not using boto3 session\n# os.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"XXX\" # needed if not using boto3 session\n\nfrom easyllm.clients import bedrock\n\nresponse = bedrock.ChatCompletion.create(\n    model='anthropic.claude-v2',\n    messages=[\n        {'role': 'user', 'content': 'Count to 100, with a comma between each number and no newlines. E.g., 1, 2, 3, ...'}\n    ],\n    stream=True\n)\n\nfor chunk in response:\n    print(chunk)\n</pre> import os  # set env for prompt builder os.environ[\"BEDROCK_PROMPT\"] = \"anthropic\" # vicuna, wizardlm, stablebeluga, open_assistant os.environ[\"AWS_REGION\"] = \"us-east-1\"  # change to your region # os.environ[\"AWS_ACCESS_KEY_ID\"] = \"XXX\" # needed if not using boto3 session # os.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"XXX\" # needed if not using boto3 session  from easyllm.clients import bedrock  response = bedrock.ChatCompletion.create(     model='anthropic.claude-v2',     messages=[         {'role': 'user', 'content': 'Count to 100, with a comma between each number and no newlines. E.g., 1, 2, 3, ...'}     ],     stream=True )  for chunk in response:     print(chunk)  <pre>10/26/2023 17:34:57 - INFO - easyllm.utils.logging - boto3 Bedrock client successfully created!\n{'id': 'hf-Je8BGADPWN', 'object': 'chat.completion.chunk', 'created': 1698334497, 'model': 'anthropic.claude-v2', 'choices': [{'index': 0, 'delta': {'role': 'assistant'}}]}\n{'id': 'hf-Je8BGADPWN', 'object': 'chat.completion.chunk', 'created': 1698334498, 'model': 'anthropic.claude-v2', 'choices': [{'index': 0, 'delta': {'content': ' Here'}}]}\n{'id': 'hf-Je8BGADPWN', 'object': 'chat.completion.chunk', 'created': 1698334498, 'model': 'anthropic.claude-v2', 'choices': [{'index': 0, 'delta': {'content': ' is counting to 100 with a comma'}}]}\n{'id': 'hf-Je8BGADPWN', 'object': 'chat.completion.chunk', 'created': 1698334498, 'model': 'anthropic.claude-v2', 'choices': [{'index': 0, 'delta': {'content': ' between each number and no newlines:\\n\\n1, 2, 3,'}}]}\n{'id': 'hf-Je8BGADPWN', 'object': 'chat.completion.chunk', 'created': 1698334499, 'model': 'anthropic.claude-v2', 'choices': [{'index': 0, 'delta': {'content': ' 4, 5, 6, 7, 8, 9, 10, 11'}}]}\n{'id': 'hf-Je8BGADPWN', 'object': 'chat.completion.chunk', 'created': 1698334499, 'model': 'anthropic.claude-v2', 'choices': [{'index': 0, 'delta': {'content': ', 12, 13, 14, 15, 16, 17, 18,'}}]}\n{'id': 'hf-Je8BGADPWN', 'object': 'chat.completion.chunk', 'created': 1698334499, 'model': 'anthropic.claude-v2', 'choices': [{'index': 0, 'delta': {'content': ' 19, 20, 21, 22, 23, 24, 25, 26, 27, 28,'}}]}\n{'id': 'hf-Je8BGADPWN', 'object': 'chat.completion.chunk', 'created': 1698334500, 'model': 'anthropic.claude-v2', 'choices': [{'index': 0, 'delta': {'content': ' 29, 30, 31, 32, 33, 34, 35, 36, 37, 38,'}}]}\n{'id': 'hf-Je8BGADPWN', 'object': 'chat.completion.chunk', 'created': 1698334500, 'model': 'anthropic.claude-v2', 'choices': [{'index': 0, 'delta': {'content': ' 39, 40, 41, 42, 43, 44, 45, 46, 47, 48,'}}]}\n{'id': 'hf-Je8BGADPWN', 'object': 'chat.completion.chunk', 'created': 1698334501, 'model': 'anthropic.claude-v2', 'choices': [{'index': 0, 'delta': {'content': ' 49, 50, 51'}}]}\n{'id': 'hf-Je8BGADPWN', 'object': 'chat.completion.chunk', 'created': 1698334501, 'model': 'anthropic.claude-v2', 'choices': [{'index': 0, 'delta': {'content': ', 52, 53,'}}]}\n{'id': 'hf-Je8BGADPWN', 'object': 'chat.completion.chunk', 'created': 1698334502, 'model': 'anthropic.claude-v2', 'choices': [{'index': 0, 'delta': {'content': ' 54, 55, 56'}}]}\n{'id': 'hf-Je8BGADPWN', 'object': 'chat.completion.chunk', 'created': 1698334503, 'model': 'anthropic.claude-v2', 'choices': [{'index': 0, 'delta': {'content': ', 57, 58, 59, 60, 61'}}]}\n{'id': 'hf-Je8BGADPWN', 'object': 'chat.completion.chunk', 'created': 1698334504, 'model': 'anthropic.claude-v2', 'choices': [{'index': 0, 'delta': {'content': ', 62, 63, 64, 65, 66'}}]}\n{'id': 'hf-Je8BGADPWN', 'object': 'chat.completion.chunk', 'created': 1698334504, 'model': 'anthropic.claude-v2', 'choices': [{'index': 0, 'delta': {'content': ', 67, 68, 69, 70, 71, 72, 73,'}}]}\n{'id': 'hf-Je8BGADPWN', 'object': 'chat.completion.chunk', 'created': 1698334504, 'model': 'anthropic.claude-v2', 'choices': [{'index': 0, 'delta': {'content': ' 74, 75, 76, 77, 78, 79, 80, 81'}}]}\n{'id': 'hf-Je8BGADPWN', 'object': 'chat.completion.chunk', 'created': 1698334505, 'model': 'anthropic.claude-v2', 'choices': [{'index': 0, 'delta': {'content': ', 82, 83, 84, 85, 86, 87, 88, 89, 90, 91'}}]}\n{'id': 'hf-Je8BGADPWN', 'object': 'chat.completion.chunk', 'created': 1698334505, 'model': 'anthropic.claude-v2', 'choices': [{'index': 0, 'delta': {'content': ', 92, 93, 94, 95, 96, 97, 98, 99, 100'}}]}\n{'id': 'hf-Je8BGADPWN', 'object': 'chat.completion.chunk', 'created': 1698334505, 'model': 'anthropic.claude-v2', 'choices': [{'index': 0, 'delta': {}}]}\n</pre> <p>As you can see above, streaming responses have a <code>delta</code> field rather than a <code>message</code> field. <code>delta</code> can hold things like:</p> <ul> <li>a role token (e.g., <code>{\"role\": \"assistant\"}</code>)</li> <li>a content token (e.g., <code>{\"content\": \"\\n\\n\"}</code>)</li> <li>nothing (e.g., <code>{}</code>), when the stream is over</li> </ul> In\u00a0[7]: Copied! <pre>import os \n# set env for prompt builder\nos.environ[\"BEDROCK_PROMPT\"] = \"anthropic\" # vicuna, wizardlm, stablebeluga, open_assistant\nos.environ[\"AWS_REGION\"] = \"us-east-1\"  # change to your region\nos.environ[\"AWS_PROFILE\"] = \"hf-sm\"  # change to your region\n# os.environ[\"AWS_ACCESS_KEY_ID\"] = \"XXX\" # needed if not using boto3 session\n# os.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"XXX\" # needed if not using boto3 session\nfrom easyllm.clients import bedrock\n\n# send a ChatCompletion request to count to 100\nresponse = bedrock.ChatCompletion.create(\n    model='anthropic.claude-v2',\n    messages=[\n        {'role': 'user', 'content': 'Count to 100, with a comma between each number and no newlines. E.g., 1, 2, 3, ...'}\n    ],\n    stream=True\n)\n\n# create variables to collect the stream of chunks\ncollected_chunks = []\ncollected_messages = []\n# iterate through the stream of events\nfor chunk in response:\n    collected_chunks.append(chunk)  # save the event response\n    chunk_message = chunk['choices'][0]['delta']  # extract the message\n    print(chunk_message.get('content', ''), end='')  # print the message\n    collected_messages.append(chunk_message)  # save the message\n    \n\n# print the time delay and text received\nfull_reply_content = ''.join([m.get('content', '') for m in collected_messages])\nprint(f\"Full conversation received: {full_reply_content}\")\n</pre> import os  # set env for prompt builder os.environ[\"BEDROCK_PROMPT\"] = \"anthropic\" # vicuna, wizardlm, stablebeluga, open_assistant os.environ[\"AWS_REGION\"] = \"us-east-1\"  # change to your region os.environ[\"AWS_PROFILE\"] = \"hf-sm\"  # change to your region # os.environ[\"AWS_ACCESS_KEY_ID\"] = \"XXX\" # needed if not using boto3 session # os.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"XXX\" # needed if not using boto3 session from easyllm.clients import bedrock  # send a ChatCompletion request to count to 100 response = bedrock.ChatCompletion.create(     model='anthropic.claude-v2',     messages=[         {'role': 'user', 'content': 'Count to 100, with a comma between each number and no newlines. E.g., 1, 2, 3, ...'}     ],     stream=True )  # create variables to collect the stream of chunks collected_chunks = [] collected_messages = [] # iterate through the stream of events for chunk in response:     collected_chunks.append(chunk)  # save the event response     chunk_message = chunk['choices'][0]['delta']  # extract the message     print(chunk_message.get('content', ''), end='')  # print the message     collected_messages.append(chunk_message)  # save the message       # print the time delay and text received full_reply_content = ''.join([m.get('content', '') for m in collected_messages]) print(f\"Full conversation received: {full_reply_content}\")  <pre> Here is counting to 100 with commas and no newlines:\n\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100Full conversation received:  Here is counting to 100 with commas and no newlines:\n\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100\n</pre>"},{"location":"examples/bedrock-stream-chat-completions/#how-to-stream-chat-completion-requests-with-amazon-bedrock","title":"How to stream Chat Completion requests with Amazon Bedrock\u00b6","text":"<p>By default, when you request a completion, the entire completion is generated before being sent back in a single response.</p> <p>If you're generating long completions, waiting for the response can take many seconds.</p> <p>To get responses sooner, you can 'stream' the completion as it's being generated. This allows you to start printing or processing the beginning of the completion before the full completion is finished.</p> <p>To stream completions, set <code>stream=True</code> when calling the chat completions or completions endpoints. This will return an object that streams back the response as data-only server-sent events. Extract chunks from the <code>delta</code> field rather than the <code>message</code> field.</p>"},{"location":"examples/bedrock-stream-chat-completions/#downsides","title":"Downsides\u00b6","text":"<p>Note that using <code>stream=True</code> in a production application makes it more difficult to moderate the content of the completions, as partial completions may be more difficult to evaluate.</p>"},{"location":"examples/bedrock-stream-chat-completions/#setup","title":"Setup\u00b6","text":"<p>Before you can use <code>easyllm</code> with Amazon Bedrock you need setup permission and access to the models. You can do this by following of the instructions below:</p> <ul> <li>https://docs.aws.amazon.com/IAM/latest/UserGuide/getting-set-up.html</li> <li>https://docs.aws.amazon.com/IAM/latest/UserGuide/troubleshoot_access-denied.html</li> <li>https://docs.aws.amazon.com/bedrock/latest/userguide/security-iam.html</li> </ul>"},{"location":"examples/bedrock-stream-chat-completions/#example-code","title":"Example code\u00b6","text":"<p>Below, this notebook shows:</p> <ol> <li>What a typical chat completion response looks like</li> <li>What a streaming chat completion response looks like</li> <li>How much time is saved by streaming a chat completion</li> </ol>"},{"location":"examples/bedrock-stream-chat-completions/#1-what-a-typical-chat-completion-response-looks-like","title":"1. What a typical chat completion response looks like\u00b6","text":"<p>With a typical ChatCompletions API call, the response is first computed and then returned all at once.</p>"},{"location":"examples/bedrock-stream-chat-completions/#3-how-much-time-is-saved-by-streaming-a-chat-completion","title":"3. How much time is saved by streaming a chat completion\u00b6","text":"<p>Now let's ask <code>meta-llama/Llama-2-70b-chat-hf</code> to count to 100 again, and see how long it takes.</p>"},{"location":"examples/chat-completion-api/","title":"How to use Chat Completion clients","text":"In\u00a0[\u00a0]: Copied! <pre># if needed, install and/or upgrade to the latest version of the EasyLLM Python library\n%pip install --upgrade easyllm\n</pre> # if needed, install and/or upgrade to the latest version of the EasyLLM Python library %pip install --upgrade easyllm  In\u00a0[6]: Copied! <pre># import the EasyLLM Python library for calling the EasyLLM API\nimport easyllm\n</pre> # import the EasyLLM Python library for calling the EasyLLM API import easyllm In\u00a0[1]: Copied! <pre>import os \n# set env for prompt builder\nos.environ[\"HUGGINGFACE_PROMPT\"] = \"llama2\" # vicuna, wizardlm, stablebeluga, open_assistant\n# os.environ[\"HUGGINGFACE_TOKEN\"] = \"hf_xxx\" \n\nfrom easyllm.clients import huggingface\n\n# Changing configuration without using environment variables\n# huggingface.api_key=\"hf_xxx\"\n# huggingface.prompt_builder = \"llama2\"\n\n\nMODEL=\"meta-llama/Llama-2-70b-chat-hf\"\n\nresponse = huggingface.ChatCompletion.create(\n    model=MODEL,\n    messages=[\n        {\"role\": \"system\", \"content\": \"\\nYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\"},\n        {\"role\": \"user\", \"content\": \"Knock knock.\"},\n        {\"role\": \"assistant\", \"content\": \"Who's there?\"},\n        {\"role\": \"user\", \"content\": \"Cat.\"},\n    ],\n      temperature=0.9,\n      top_p=0.6,\n      max_tokens=1024,\n)\nresponse\n</pre> import os  # set env for prompt builder os.environ[\"HUGGINGFACE_PROMPT\"] = \"llama2\" # vicuna, wizardlm, stablebeluga, open_assistant # os.environ[\"HUGGINGFACE_TOKEN\"] = \"hf_xxx\"   from easyllm.clients import huggingface  # Changing configuration without using environment variables # huggingface.api_key=\"hf_xxx\" # huggingface.prompt_builder = \"llama2\"   MODEL=\"meta-llama/Llama-2-70b-chat-hf\"  response = huggingface.ChatCompletion.create(     model=MODEL,     messages=[         {\"role\": \"system\", \"content\": \"\\nYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\"},         {\"role\": \"user\", \"content\": \"Knock knock.\"},         {\"role\": \"assistant\", \"content\": \"Who's there?\"},         {\"role\": \"user\", \"content\": \"Cat.\"},     ],       temperature=0.9,       top_p=0.6,       max_tokens=1024, ) response Out[1]: <pre>{'id': 'hf-lt8HWKZn-O',\n 'object': 'chat.completion',\n 'created': 1695106434,\n 'model': 'meta-llama/Llama-2-70b-chat-hf',\n 'choices': [{'index': 0,\n   'message': {'role': 'assistant', 'content': ' Cat who?'},\n   'finish_reason': 'eos_token'}],\n 'usage': {'prompt_tokens': 149, 'completion_tokens': 5, 'total_tokens': 154}}</pre> <p>As you can see, the response object has a few fields:</p> <ul> <li><code>id</code>: the ID of the request</li> <li><code>object</code>: the type of object returned (e.g., <code>chat.completion</code>)</li> <li><code>created</code>: the timestamp of the request</li> <li><code>model</code>: the full name of the model used to generate the response</li> <li><code>usage</code>: the number of tokens used to generate the replies, counting prompt, completion, and total</li> <li><code>choices</code>: a list of completion objects (only one, unless you set <code>n</code> greater than 1)<ul> <li><code>message</code>: the message object generated by the model, with <code>role</code> and <code>content</code></li> <li><code>finish_reason</code>: the reason the model stopped generating text (either <code>stop</code>, or <code>length</code> if <code>max_tokens</code> limit was reached)</li> <li><code>index</code>: the index of the completion in the list of choices</li> </ul> </li> </ul> <p>Extract just the reply with:</p> In\u00a0[3]: Copied! <pre>print(response['choices'][0]['message']['content'])\n</pre> print(response['choices'][0]['message']['content']) <pre> Cat who?\n</pre> <p>Even non-conversation-based tasks can fit into the chat format, by placing the instruction in the first user message.</p> <p>For example, to ask the model to explain asynchronous programming in the style of the pirate Blackbeard, we can structure conversation as follows:</p> In\u00a0[4]: Copied! <pre># example with a system message\nresponse = huggingface.ChatCompletion.create(\n    model=MODEL,\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"Explain asynchronous programming in the style of math teacher.\"},\n    ],\n)\n\nprint(response['choices'][0]['message']['content'])\n</pre> # example with a system message response = huggingface.ChatCompletion.create(     model=MODEL,     messages=[         {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},         {\"role\": \"user\", \"content\": \"Explain asynchronous programming in the style of math teacher.\"},     ], )  print(response['choices'][0]['message']['content'])  <pre> Hello, my dear students! Today, we're going to learn about a fascinating topic that will help us understand how to make our programs more efficient and responsive: asynchronous programming.\n\nImagine you're working on a project with a group of people, and you need to finish your part before others can start theirs. But, you're waiting for someone else to finish their part so you can start yours. This is similar to how asynchronous programming works.\n\nIn asynchronous programming, we break down a program into smaller parts called \"tasks.\" These tasks can run independently, without blocking other tasks from running. This means that if one task is waiting for something to happen, like a response from a server or a user input, other tasks can keep running in the meantime.\n\nLet's use a simple example to illustrate this. Imagine you're making a sandwich. You need to put the bread slices together, add the filling, and then put the sandwich in the fridge to chill. But, you can't start making the sandwich until the bread is toasted, and you can't put the sandwich in the fridge until it's assembled.\n\nIn this scenario, toasting the bread and assembling the sandwich are two separate tasks. If we were to do them synchronously, we would do them one after the other, like this:\n\n1. Toast the bread\n2. Assemble the sandwich\n3. Put the sandwich in the fridge\n\nBut, with asynchronous programming, we can do them simultaneously, like this:\n\n1. Toast the bread (starts)\n2. Assemble the sandwich (starts)\n3. Toast the bread (finishes)\n4. Put the sandwich in the fridge\n\nBy doing tasks simultaneously, we can save time and make our program more efficient. But, we need to be careful not to get confused about the order in which things happen. That's why we use special tools, like \"promises\" and \"callbacks,\" to keep track of everything.\n\nSo, my dear students, I hope this helps you understand asynchronous programming a bit better. Remember, it's all about breaking down a program into smaller, independent tasks that can run simultaneously, making our programs more efficient and responsive. Now, go forth and create some amazing programs!\n</pre> In\u00a0[5]: Copied! <pre># example without a system message and debug flag on:\nresponse = huggingface.ChatCompletion.create(\n    model=MODEL,\n    messages=[\n        {\"role\": \"user\", \"content\": \"Explain asynchronous programming in the style of the pirate Blackbeard.\"},\n    ],\n    debug=True,\n)\n\nprint(response['choices'][0]['message']['content'])\n</pre> # example without a system message and debug flag on: response = huggingface.ChatCompletion.create(     model=MODEL,     messages=[         {\"role\": \"user\", \"content\": \"Explain asynchronous programming in the style of the pirate Blackbeard.\"},     ],     debug=True, )  print(response['choices'][0]['message']['content'])  <pre>08/04/2023 08:16:57 - DEBUG - easyllm.utils - Prompt sent to model will be:\n&lt;s&gt;[INST] Explain asynchronous programming in the style of the pirate Blackbeard. [/INST]\n08/04/2023 08:16:57 - DEBUG - easyllm.utils - Url:\nhttps://api-inference.huggingface.co/models/meta-llama/Llama-2-70b-chat-hf\n08/04/2023 08:16:57 - DEBUG - easyllm.utils - Stop sequences:\n[]\n08/04/2023 08:16:57 - DEBUG - easyllm.utils - Generation parameters:\n{'do_sample': True, 'return_full_text': False, 'max_new_tokens': 1024, 'top_p': 0.6, 'temperature': 0.9, 'stop_sequences': [], 'repetition_penalty': 1.0, 'top_k': 10, 'seed': 42}\n08/04/2023 08:16:57 - DEBUG - easyllm.utils - Response at index 0:\nindex=0 message=ChatMessage(role='assistant', content=' Ahoy matey! Yer lookin\\' fer a tale of asynchronous programming, eh? Well, settle yerself down with a pint o\\' grog and listen close, for Blackbeard\\'s got a story fer ye.\\n\\nAsynchronous programming, me hearties, be like sailin\\' a ship through treacherous waters. Ye gotta keep yer wits about ye, and watch out fer the hidden dangers that lie beneath the surface.\\n\\nImagine ye\\'re sailin\\' along, and suddenly, out o\\' the blue, a great storm brews up. The winds howl, the waves crash, and yer ship takes on water. Now, ye gotta act fast, or ye\\'ll be sent to Davy Jones\\' locker!\\n\\nBut, me hearties, ye can\\'t just abandon ship. Ye gotta batten down the hatches, and ride out the storm. And that\\'s where asynchronous programming comes in.\\n\\nAsynchronous programming be like haulin\\' up the sails, and lettin\\' the wind do the work fer ye. Ye don\\'t have to worry about the details o\\' how the wind\\'s blowin\\', or the waves crashin\\', ye just gotta keep yer ship pointed in the right direction, and let nature take its course.\\n\\nNow, I know what ye\\'re thinkin\\', \"Blackbeard, how do I know when me ship\\'s gonna make it through the storm?\" And that, me hearties, be the beauty o\\' asynchronous programming. Ye don\\'t have to know! Ye just have to trust that the winds o\\' change will carry ye through, and ye\\'ll make it to the other side, all in one piece.\\n\\nBut, me hearties, don\\'t ye be thinkin\\' this be easy. Asynchronous programming be like navigatin\\' through treacherous waters, with a crew o\\' mutinous code, and a hull full o\\' bugs. Ye gotta be prepared fer the unexpected, and have a stout heart, or ye\\'ll be walkin\\' the plank!\\n\\nSo, me hearties, there ye have it. Asynchronous programming in the style o\\' Blackbeard. May the winds o\\' change blow in yer favor, and may yer code always be free o\\' bugs! Arrr!') finish_reason='eos_token'\n Ahoy matey! Yer lookin' fer a tale of asynchronous programming, eh? Well, settle yerself down with a pint o' grog and listen close, for Blackbeard's got a story fer ye.\n\nAsynchronous programming, me hearties, be like sailin' a ship through treacherous waters. Ye gotta keep yer wits about ye, and watch out fer the hidden dangers that lie beneath the surface.\n\nImagine ye're sailin' along, and suddenly, out o' the blue, a great storm brews up. The winds howl, the waves crash, and yer ship takes on water. Now, ye gotta act fast, or ye'll be sent to Davy Jones' locker!\n\nBut, me hearties, ye can't just abandon ship. Ye gotta batten down the hatches, and ride out the storm. And that's where asynchronous programming comes in.\n\nAsynchronous programming be like haulin' up the sails, and lettin' the wind do the work fer ye. Ye don't have to worry about the details o' how the wind's blowin', or the waves crashin', ye just gotta keep yer ship pointed in the right direction, and let nature take its course.\n\nNow, I know what ye're thinkin', \"Blackbeard, how do I know when me ship's gonna make it through the storm?\" And that, me hearties, be the beauty o' asynchronous programming. Ye don't have to know! Ye just have to trust that the winds o' change will carry ye through, and ye'll make it to the other side, all in one piece.\n\nBut, me hearties, don't ye be thinkin' this be easy. Asynchronous programming be like navigatin' through treacherous waters, with a crew o' mutinous code, and a hull full o' bugs. Ye gotta be prepared fer the unexpected, and have a stout heart, or ye'll be walkin' the plank!\n\nSo, me hearties, there ye have it. Asynchronous programming in the style o' Blackbeard. May the winds o' change blow in yer favor, and may yer code always be free o' bugs! Arrr!\n</pre> In\u00a0[6]: Copied! <pre># An example of a faked few-shot conversation to prime the model into translating business jargon to simpler speech\nresponse = huggingface.ChatCompletion.create(\n    model=MODEL,\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful, pattern-following assistant.\"},\n        {\"role\": \"user\", \"content\": \"Help me translate the following corporate jargon into plain English.\"},\n        {\"role\": \"assistant\", \"content\": \"Sure, I'd be happy to!\"},\n        {\"role\": \"user\", \"content\": \"New synergies will help drive top-line growth.\"},\n        {\"role\": \"assistant\", \"content\": \"Things working well together will increase revenue.\"},\n        {\"role\": \"user\", \"content\": \"Let's circle back when we have more bandwidth to touch base on opportunities for increased leverage.\"},\n        {\"role\": \"assistant\", \"content\": \"Let's talk later when we're less busy about how to do better.\"},\n        {\"role\": \"user\", \"content\": \"This late pivot means we don't have time to boil the ocean for the client deliverable.\"},\n    ],\n)\n\nprint(response[\"choices\"][0][\"message\"][\"content\"])\n</pre> # An example of a faked few-shot conversation to prime the model into translating business jargon to simpler speech response = huggingface.ChatCompletion.create(     model=MODEL,     messages=[         {\"role\": \"system\", \"content\": \"You are a helpful, pattern-following assistant.\"},         {\"role\": \"user\", \"content\": \"Help me translate the following corporate jargon into plain English.\"},         {\"role\": \"assistant\", \"content\": \"Sure, I'd be happy to!\"},         {\"role\": \"user\", \"content\": \"New synergies will help drive top-line growth.\"},         {\"role\": \"assistant\", \"content\": \"Things working well together will increase revenue.\"},         {\"role\": \"user\", \"content\": \"Let's circle back when we have more bandwidth to touch base on opportunities for increased leverage.\"},         {\"role\": \"assistant\", \"content\": \"Let's talk later when we're less busy about how to do better.\"},         {\"role\": \"user\", \"content\": \"This late pivot means we don't have time to boil the ocean for the client deliverable.\"},     ], )  print(response[\"choices\"][0][\"message\"][\"content\"])  <pre>08/04/2023 08:16:57 - DEBUG - easyllm.utils - Prompt sent to model will be:\n&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\nYou are a helpful, pattern-following assistant.\n&lt;&lt;/SYS&gt;&gt;\n\nHelp me translate the following corporate jargon into plain English. [/INST] Sure, I'd be happy to!&lt;/s&gt;&lt;s&gt;[INST] New synergies will help drive top-line growth. [/INST] Things working well together will increase revenue.&lt;/s&gt;&lt;s&gt;[INST] Let's circle back when we have more bandwidth to touch base on opportunities for increased leverage. [/INST] Let's talk later when we're less busy about how to do better.&lt;/s&gt;&lt;s&gt;[INST] This late pivot means we don't have time to boil the ocean for the client deliverable. [/INST]\n08/04/2023 08:16:57 - DEBUG - easyllm.utils - Url:\nhttps://api-inference.huggingface.co/models/meta-llama/Llama-2-70b-chat-hf\n08/04/2023 08:16:57 - DEBUG - easyllm.utils - Stop sequences:\n[]\n08/04/2023 08:16:57 - DEBUG - easyllm.utils - Generation parameters:\n{'do_sample': True, 'return_full_text': False, 'max_new_tokens': 1024, 'top_p': 0.6, 'temperature': 0.9, 'stop_sequences': [], 'repetition_penalty': 1.0, 'top_k': 10, 'seed': 42}\n08/04/2023 08:16:57 - DEBUG - easyllm.utils - Response at index 0:\nindex=0 message=ChatMessage(role='assistant', content=\" We've changed direction too late to do a complete job for the client.\") finish_reason='eos_token'\n We've changed direction too late to do a complete job for the client.\n</pre> <p>Not every attempt at engineering conversations will succeed at first.</p> <p>If your first attempts fail, don't be afraid to experiment with different ways of priming or conditioning the model.</p> <p>As an example, one developer discovered an increase in accuracy when they inserted a user message that said \"Great job so far, these have been perfect\" to help condition the model into providing higher quality responses.</p> <p>For more ideas on how to lift the reliability of the models, consider reading our guide on techniques to increase reliability. It was written for non-chat models, but many of its principles still apply.</p>"},{"location":"examples/chat-completion-api/#how-to-use-chat-completion-clients","title":"How to use Chat Completion clients\u00b6","text":"<p>EasyLLM can be used as an abstract layer to replace <code>gpt-3.5-turbo</code> and <code>gpt-4</code> with open source models.</p> <p>You can change your own applications from the OpenAI API, by simply changing the client.</p> <p>Chat models take a series of messages as input, and return an AI-written message as output.</p> <p>This guide illustrates the chat format with a few example API calls.</p>"},{"location":"examples/chat-completion-api/#1-import-the-easyllm-library","title":"1. Import the easyllm library\u00b6","text":""},{"location":"examples/chat-completion-api/#2-an-example-chat-api-call","title":"2. An example chat API call\u00b6","text":"<p>A chat API call has two required inputs:</p> <ul> <li><code>model</code>: the name of the model you want to use (e.g., <code>meta-llama/Llama-2-70b-chat-hf</code>) or leave it empty to just call the api</li> <li><code>messages</code>: a list of message objects, where each object has two required fields:<ul> <li><code>role</code>: the role of the messenger (either <code>system</code>, <code>user</code>, or <code>assistant</code>)</li> <li><code>content</code>: the content of the message (e.g., <code>Write me a beautiful poem</code>)</li> </ul> </li> </ul> <p>Compared to OpenAI api is the <code>huggingface</code> module also exposing a <code>prompt_builder</code> and <code>stop_sequences</code> parameter you can use to customize the prompt and stop sequences. The EasyLLM package comes with prompt builder utilities.</p> <p>Let's look at an example chat API calls to see how the chat format works in practice.</p>"},{"location":"examples/chat-completion-api/#3-few-shot-prompting","title":"3. Few-shot prompting\u00b6","text":"<p>In some cases, it's easier to show the model what you want rather than tell the model what you want.</p> <p>One way to show the model what you want is with faked example messages.</p> <p>For example:</p>"},{"location":"examples/data-filter/","title":"How to use EasyLLM Quality data filters","text":"In\u00a0[\u00a0]: Copied! <pre>!pip install \"easyllm[data]\" --upgrade\n</pre> !pip install \"easyllm[data]\" --upgrade In\u00a0[2]: Copied! <pre>from easyllm.data.filters import PerplexityFilter\n\nppl = PerplexityFilter(\"en\",min_threshold=10,max_threshold=1000)\n\n# Get perplexity\nprint(ppl.model.get_perplexity(\"I am very perplexed\"))\n# 341.3 (low perplexity, since sentence style is formal and with no grammar mistakes)\n\nprint(ppl.model.get_perplexity(\"im hella trippin\"))\n# 46793.5 (high perplexity, since the sentence is colloquial and contains grammar mistakes)\n\n# testing the filter\nassert ppl(\"I am very perplexed\") == False\n</pre> from easyllm.data.filters import PerplexityFilter  ppl = PerplexityFilter(\"en\",min_threshold=10,max_threshold=1000)  # Get perplexity print(ppl.model.get_perplexity(\"I am very perplexed\")) # 341.3 (low perplexity, since sentence style is formal and with no grammar mistakes)  print(ppl.model.get_perplexity(\"im hella trippin\")) # 46793.5 (high perplexity, since the sentence is colloquial and contains grammar mistakes)  # testing the filter assert ppl(\"I am very perplexed\") == False  <pre>341.3\n46793.5\n</pre> In\u00a0[1]: Copied! <pre>from easyllm.data.filters import NonAlphaNumericFilter\n\nnam = NonAlphaNumericFilter()\n\n# not filtered\nassert nam(\"This is a test\") == False\n\n# filtered\nassert nam(\"This is a test!!!!!!!\") == True\n</pre> from easyllm.data.filters import NonAlphaNumericFilter  nam = NonAlphaNumericFilter()  # not filtered assert nam(\"This is a test\") == False  # filtered assert nam(\"This is a test!!!!!!!\") == True  In\u00a0[1]: Copied! <pre>from easyllm.data.filters import SymbolToWordFilter\n\nstw = SymbolToWordFilter()\n\nassert stw(\"This is a test\") == False\n\nassert stw(\"spam#spam#spam#spam#spam#spam#spam#spam\") == True\n</pre> from easyllm.data.filters import SymbolToWordFilter  stw = SymbolToWordFilter()  assert stw(\"This is a test\") == False  assert stw(\"spam#spam#spam#spam#spam#spam#spam#spam\") == True In\u00a0[1]: Copied! <pre>from easyllm.data.filters import DigitToCharacter\n\nntw = DigitToCharacter()\n\nassert ntw(\"Hello 123 world 456 this text 789 contains 1234 numbers more words\") == False\n\nassert ntw(\"Hello 34534 34534 \") == True\n</pre> from easyllm.data.filters import DigitToCharacter  ntw = DigitToCharacter()  assert ntw(\"Hello 123 world 456 this text 789 contains 1234 numbers more words\") == False  assert ntw(\"Hello 34534 34534 \") == True  In\u00a0[2]: Copied! <pre>from easyllm.data.filters import UrlRatioFilter \n\nur = UrlRatioFilter()\n\nassert ur(\"https://www.google.com\") == True\n\nassert ur(\"Example text with some urls http://www.example.com and more text https://www.example2.com and more text\") == False\n</pre> from easyllm.data.filters import UrlRatioFilter   ur = UrlRatioFilter()  assert ur(\"https://www.google.com\") == True  assert ur(\"Example text with some urls http://www.example.com and more text https://www.example2.com and more text\") == False In\u00a0[1]: Copied! <pre>from easyllm.data.filters import BulletpointRatioFilter\n\nbr = BulletpointRatioFilter()\n\nassert br(\"This is a text with \\n- some bullets but\\nnot all\") == False\n\nassert br(\"- some bullets and\\n- some more\") == True\n</pre> from easyllm.data.filters import BulletpointRatioFilter  br = BulletpointRatioFilter()  assert br(\"This is a text with \\n- some bullets but\\nnot all\") == False  assert br(\"- some bullets and\\n- some more\") == True In\u00a0[2]: Copied! <pre>from easyllm.data.filters import WhitespaceRatioFilter\n\nwr = WhitespaceRatioFilter()\n\nassert wr(\"This is a test\") == False\n\nassert wr(\"Hello world!      This text has    extra whitespace.\") == True\n</pre> from easyllm.data.filters import WhitespaceRatioFilter  wr = WhitespaceRatioFilter()  assert wr(\"This is a test\") == False  assert wr(\"Hello world!      This text has    extra whitespace.\") == True In\u00a0[1]: Copied! <pre>from easyllm.data.filters import ParenthesesRationFilter\n\npr = ParenthesesRationFilter()\n\nassert pr(\"This is a normal sentence\") == False\n\nassert pr(\"This a (with ) ] {(e)\") == True\n</pre> from easyllm.data.filters import ParenthesesRationFilter  pr = ParenthesesRationFilter()  assert pr(\"This is a normal sentence\") == False  assert pr(\"This a (with ) ] {(e)\") == True In\u00a0[2]: Copied! <pre>from easyllm.data.filters import LongWordFilter\n\nlw = LongWordFilter()\n\nassert lw(\"This is a test\") == False\n\nassert lw(f\"This is a test with a {'longword'*500}\") == True\n</pre> from easyllm.data.filters import LongWordFilter  lw = LongWordFilter()  assert lw(\"This is a test\") == False  assert lw(f\"This is a test with a {'longword'*500}\") == True In\u00a0[1]: Copied! <pre>from easyllm.data.filters import LengthFilter\n\nl = LengthFilter(min_length=1, max_length=100)\n\nassert l(\"hello world\") == False\n\nassert l(\"hello world \" * 100) == True\n</pre> from easyllm.data.filters import LengthFilter  l = LengthFilter(min_length=1, max_length=100)  assert l(\"hello world\") == False  assert l(\"hello world \" * 100) == True In\u00a0[1]: Copied! <pre>from easyllm.data.filters import RepeatedLinesFilter, RepeatedParagraphFilter\n\nrl = RepeatedLinesFilter()\nrp = RepeatedParagraphFilter()\n\nassert rl(\"hello\\nworld\") == False\nassert rl(\"hello\\nhello\\nhello\\nhello\") == True\n\nassert rp(\"hello\\n\\nworld\") == False\nassert rp(\"hello\\n\\nhello\\n\\nhello\\n\\nhello\") == True\n</pre> from easyllm.data.filters import RepeatedLinesFilter, RepeatedParagraphFilter  rl = RepeatedLinesFilter() rp = RepeatedParagraphFilter()  assert rl(\"hello\\nworld\") == False assert rl(\"hello\\nhello\\nhello\\nhello\") == True  assert rp(\"hello\\n\\nworld\") == False assert rp(\"hello\\n\\nhello\\n\\nhello\\n\\nhello\") == True In\u00a0[1]: Copied! <pre>from easyllm.data.filters import TopNGramsFilter\n\ntng = TopNGramsFilter()\n\nassert tng(\"This is a test for a longer sentence\") == False \n\nassert tng(\"The quick brown fox jumps over the lazy dog The quick brown\") == True\n</pre> from easyllm.data.filters import TopNGramsFilter  tng = TopNGramsFilter()  assert tng(\"This is a test for a longer sentence\") == False   assert tng(\"The quick brown fox jumps over the lazy dog The quick brown\") == True In\u00a0[3]: Copied! <pre>from easyllm.data.filters import PunctuationFilter, EllipsisFilter\n\npf = PunctuationFilter()\n\nassert pf(\"This is a sentence.\") == False\n\nassert pf(\"This is a sentence\\n But is not one.\\nNo oneyet.\") == True\n\nef = EllipsisFilter()\n\nassert ef(\"This is a sentence.\") == False\n\nassert ef(\"This is a sentence\\n But is not one....\") == True\n</pre> from easyllm.data.filters import PunctuationFilter, EllipsisFilter  pf = PunctuationFilter()  assert pf(\"This is a sentence.\") == False  assert pf(\"This is a sentence\\n But is not one.\\nNo oneyet.\") == True  ef = EllipsisFilter()  assert ef(\"This is a sentence.\") == False  assert ef(\"This is a sentence\\n But is not one....\") == True In\u00a0[1]: Copied! <pre>from easyllm.data.filters import CommonWordFilter\n\ncw = CommonWordFilter()\n\nassert cw(\"This is a sentence with a common word.\") == False\n\nassert cw(\"cat dog mouse\") == True\n</pre> from easyllm.data.filters import CommonWordFilter  cw = CommonWordFilter()  assert cw(\"This is a sentence with a common word.\") == False  assert cw(\"cat dog mouse\") == True In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"examples/data-filter/#how-to-use-easyllm-quality-data-filters","title":"How to use EasyLLM Quality data filters\u00b6","text":"<p>EasyLLMs <code>data</code> package adds quality filters for preprocessing text data for improved pretraining.</p>"},{"location":"examples/data-filter/#1-perplexity-filtering","title":"1. Perplexity filtering\u00b6","text":"<p>Perplexity filtering can be used to improve model quality, coherence, and training efficiency by removing confusing text segments and focusing model learning on more standard, comprehensible language. Perplexity filtering is implemented using <code>KenLM</code> models trained on wikipedia. You just need to provide your language id, e.g. <code>de</code> and your perplexity <code>min_threshold</code> and <code>max_threshold</code> the filter will return <code>True</code> if the perplexity of the text outside of the threshold <code>False</code> otherwise.</p>"},{"location":"examples/data-filter/#nonalphanumericfilter","title":"NonAlphaNumericFilter\u00b6","text":"<p>The <code>NonAlphaNumericFilter</code> removes documents based on the number of non-alphanumeric characters in the document. Based on Gopher (Rae et al., 2021), if the document has more then 20% non-alphanumeric characters, it is removed.</p>"},{"location":"examples/data-filter/#symboltowordfilter","title":"SymbolToWordFilter\u00b6","text":"<p>The <code>SymbolToWordFilter</code> removes any document with a symbol-to-word ratio greater than 0.1 for either the hash symbol or the ellipsis. Based on Gopher (Rae et al., 2021)</p>"},{"location":"examples/data-filter/#numberstocharacterfilter","title":"NumbersToCharacterFilter\u00b6","text":"<p>The <code>NumbersToCharacterFilter</code> removes any document where the 20% of the document are numbers.</p>"},{"location":"examples/data-filter/#urlratiofilter","title":"UrlRatioFilter\u00b6","text":"<p>The <code>UrlRatioFilter</code> removes any document where 20% of the document is a URL.</p>"},{"location":"examples/data-filter/#bulletpointratiofilter","title":"BulletpointRatioFilter\u00b6","text":"<p>The <code>BulletpointRatioFilter</code> removes documents that have more than 90% bulletpoints. Based on Gopher (Rae et al., 2021)</p>"},{"location":"examples/data-filter/#whitespaceratiofilter","title":"WhitespaceRatioFilter\u00b6","text":"<p>The <code>WhitespaceRatioFilter</code> is a filter that removes documents that more than 25% of the text is whitespace.</p>"},{"location":"examples/data-filter/#parenthesesrationfilter","title":"ParenthesesRationFilter\u00b6","text":"<p>The <code>ParenthesesRationFilter</code> is a filter that removes all sentences that have a parentheses ratio greater than 10%.</p>"},{"location":"examples/data-filter/#longwordfilter","title":"LongWordFilter\u00b6","text":"<p>The <code>LongWordFilter</code> is a filter that removes documents that include words longer &gt; 1000 character, e.g. js minfied files.</p>"},{"location":"examples/data-filter/#lengthfilter","title":"LengthFilter\u00b6","text":"<p>The <code>LengthFilter</code> removes documents below or above a certain number of words. Not tokens since its more expensive to compute.</p>"},{"location":"examples/data-filter/#repeatedparagraphfilter-repeatedlinesfilter","title":"RepeatedParagraphFilter, RepeatedLinesFilter\u00b6","text":"<p>The <code>RepeatedParagraphFilter</code> &amp; <code>RepeatedLinesFilter</code> remove documents which have more than 30% repeated lines or paragraphs. Based on Gopher (Rae et al., 2021)</p>"},{"location":"examples/data-filter/#topngramsfilter","title":"TopNGramsFilter\u00b6","text":"<p>The <code>TopNGramsFilter</code> removes the document if the top n-gram makes more than 20% of the document.</p>"},{"location":"examples/data-filter/#punctuationfilter-ellipsisfilter","title":"PunctuationFilter &amp; EllipsisFilter\u00b6","text":"<p>The <code>PunctuationFilter</code> &amp; <code>EllipsisFilter</code> removes the document if more than 15% of the \"linebreaks\" don't contain any punctuation or if more than 30% of the \"linebreaks\" contain an ellipsis.</p>"},{"location":"examples/data-filter/#commonwordfilter","title":"CommonWordFilter\u00b6","text":"<p>The <code>CommonWordFilter</code> removes documents if they don't include atleast 2 common words.</p>"},{"location":"examples/falcon-180b-chat/","title":"How to use Chat Completion clients","text":"In\u00a0[\u00a0]: Copied! <pre># if needed, install and/or upgrade to the latest version of the EasyLLM Python library\n%pip install --upgrade easyllm\n</pre> # if needed, install and/or upgrade to the latest version of the EasyLLM Python library %pip install --upgrade easyllm  In\u00a0[4]: Copied! <pre># import the EasyLLM Python library for calling the EasyLLM API\nimport easyllm\n</pre> # import the EasyLLM Python library for calling the EasyLLM API import easyllm In\u00a0[1]: Copied! <pre>import os \n# set env for prompt builder\nos.environ[\"HUGGINGFACE_PROMPT\"] = \"falcon\" # vicuna, wizardlm, stablebeluga, open_assistant\n# os.environ[\"HUGGINGFACE_TOKEN\"] = \"hf_xxx\" \n\nfrom easyllm.clients import huggingface\nfrom easyllm.prompt_utils.falcon import falcon_stop_sequences\n\nMODEL=\"tiiuae/falcon-180B-chat\"\n\nresponse = huggingface.ChatCompletion.create(\n    model=MODEL,\n    messages=[\n        {\"role\": \"system\", \"content\": \"\\nYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\"},\n        {\"role\": \"user\", \"content\": \"Knock knock.\"},\n        {\"role\": \"assistant\", \"content\": \"Who's there?\"},\n        {\"role\": \"user\", \"content\": \"Cat.\"},\n    ],\n      temperature=0.9,\n      top_p=0.6,\n      max_tokens=1024,\n      stop=falcon_stop_sequences,\n)\nresponse\n</pre> import os  # set env for prompt builder os.environ[\"HUGGINGFACE_PROMPT\"] = \"falcon\" # vicuna, wizardlm, stablebeluga, open_assistant # os.environ[\"HUGGINGFACE_TOKEN\"] = \"hf_xxx\"   from easyllm.clients import huggingface from easyllm.prompt_utils.falcon import falcon_stop_sequences  MODEL=\"tiiuae/falcon-180B-chat\"  response = huggingface.ChatCompletion.create(     model=MODEL,     messages=[         {\"role\": \"system\", \"content\": \"\\nYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\"},         {\"role\": \"user\", \"content\": \"Knock knock.\"},         {\"role\": \"assistant\", \"content\": \"Who's there?\"},         {\"role\": \"user\", \"content\": \"Cat.\"},     ],       temperature=0.9,       top_p=0.6,       max_tokens=1024,       stop=falcon_stop_sequences, ) response Out[1]: <pre>{'id': 'hf-ceVG8KGm04',\n 'object': 'chat.completion',\n 'created': 1695106309,\n 'model': 'tiiuae/falcon-180B-chat',\n 'choices': [{'index': 0,\n   'message': {'role': 'assistant',\n    'content': \"*Knock knock* Who's there? Cat. Cat who? Cat got your tongue?\\nUser:\"},\n   'finish_reason': 'stop_sequence'}],\n 'usage': {'prompt_tokens': 144, 'completion_tokens': 23, 'total_tokens': 167}}</pre> <p>As you can see, the response object has a few fields:</p> <ul> <li><code>id</code>: the ID of the request</li> <li><code>object</code>: the type of object returned (e.g., <code>chat.completion</code>)</li> <li><code>created</code>: the timestamp of the request</li> <li><code>model</code>: the full name of the model used to generate the response</li> <li><code>usage</code>: the number of tokens used to generate the replies, counting prompt, completion, and total</li> <li><code>choices</code>: a list of completion objects (only one, unless you set <code>n</code> greater than 1)<ul> <li><code>message</code>: the message object generated by the model, with <code>role</code> and <code>content</code></li> <li><code>finish_reason</code>: the reason the model stopped generating text (either <code>stop</code>, or <code>length</code> if <code>max_tokens</code> limit was reached)</li> <li><code>index</code>: the index of the completion in the list of choices</li> </ul> </li> </ul> <p>Extract just the reply with:</p> In\u00a0[2]: Copied! <pre>print(response['choices'][0]['message']['content'])\n</pre> print(response['choices'][0]['message']['content']) <pre>*Knock knock* Who's there? Cat. Cat who? Cat got your tongue?\nUser:\n</pre> <p>Even non-conversation-based tasks can fit into the chat format, by placing the instruction in the first user message.</p> <p>For example, to ask the model to explain asynchronous programming in the style of the pirate Blackbeard, we can structure conversation as follows:</p> In\u00a0[3]: Copied! <pre># example with a system message\nresponse = huggingface.ChatCompletion.create(\n    model=MODEL,\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"Explain asynchronous programming in the style of math teacher.\"},\n    ],\n    stop=falcon_stop_sequences,\n)\n\nprint(response['choices'][0]['message']['content'])\n</pre> # example with a system message response = huggingface.ChatCompletion.create(     model=MODEL,     messages=[         {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},         {\"role\": \"user\", \"content\": \"Explain asynchronous programming in the style of math teacher.\"},     ],     stop=falcon_stop_sequences, )  print(response['choices'][0]['message']['content'])  <pre>\nAsynchronous Programming: A Mathematical Approach\n\nGood day, class! Today we're going to discuss a fascinating topic in the world of programming - asynchronous programming. Now, you might be wondering what this has to do with math. Well, just like how mathematical operations can sometimes be synchronous or asynchronous, so too can computer programs.\n\nLet's start by defining our terms. Synchronous processes are those that happen one after another, in a predictable sequence. For example, if you were to add two numbers together, then multiply the result by another number, these operations would typically happen synchronously \u2013 the addition occurs first, followed by the multiplication.\n\nAsynchronous processes, on the other hand, don't necessarily follow such a strict order. They're more like parallel lines in geometry \u2013 they can run alongside each other independently, without waiting for one another to finish. In programming, this means that multiple tasks can be performed at the same time, without one task blocking another from starting.\n\nSo why is this useful? Well, imagine you're working on a complex mathematical problem that requires several calculations. If you were to perform these calculations synchronously, you'd have to wait for each calculation to finish before starting the next one. This could take quite some time, especially if your calculations are dependent on external factors such as user input or network latency.\n\nWith asynchronous programming, however, you can perform multiple calculations simultaneously. This means that while one calculation is waiting for user input, another can continue processing data from a different source. As a result, your overall computation time is reduced, making your program more efficient and responsive.\n\nOf course, there are challenges involved in asynchronous programming, much like solving an intricate mathematical puzzle. One major issue is ensuring that all asynchronous tasks complete successfully, even if they encounter errors along the way. This requires careful planning and error handling, similar to how you would approach solving a complex equation.\n\nIn conclusion, asynchronous programming is a powerful tool in the programmer's toolkit, much like advanced mathematical concepts are essential for solving complex problems. By understanding the principles behind asynchronous processes, you can create more efficient and responsive programs, ready to tackle any challenge that comes their way.\n\nNow, let's put this knowledge into practice with some coding exercises, shall we?\n</pre>"},{"location":"examples/falcon-180b-chat/#how-to-use-chat-completion-clients","title":"How to use Chat Completion clients\u00b6","text":"<p>EasyLLM can be used as an abstract layer to replace <code>gpt-3.5-turbo</code> and <code>gpt-4</code> with open source models.</p> <p>You can change your own applications from the OpenAI API, by simply changing the client.</p> <p>Chat models take a series of messages as input, and return an AI-written message as output.</p> <p>This guide illustrates the chat format with a few example API calls.</p>"},{"location":"examples/falcon-180b-chat/#1-import-the-easyllm-library","title":"1. Import the easyllm library\u00b6","text":""},{"location":"examples/falcon-180b-chat/#2-an-example-chat-api-call","title":"2. An example chat API call\u00b6","text":"<p>A chat API call has two required inputs:</p> <ul> <li><code>model</code>: the name of the model you want to use (e.g., <code>meta-llama/Llama-2-70b-chat-hf</code>) or leave it empty to just call the api</li> <li><code>messages</code>: a list of message objects, where each object has two required fields:<ul> <li><code>role</code>: the role of the messenger (either <code>system</code>, <code>user</code>, or <code>assistant</code>)</li> <li><code>content</code>: the content of the message (e.g., <code>Write me a beautiful poem</code>)</li> </ul> </li> </ul> <p>Compared to OpenAI api is the <code>huggingface</code> module also exposing a <code>prompt_builder</code> and <code>stop_sequences</code> parameter you can use to customize the prompt and stop sequences. The EasyLLM package comes with prompt builder utilities.</p> <p>Let's look at an example chat API calls to see how the chat format works in practice.</p>"},{"location":"examples/get-embeddings/","title":"Get embeddings","text":"In\u00a0[\u00a0]: Copied! <pre># if needed, install and/or upgrade to the latest version of the OpenAI Python library\n%pip install --upgrade easyllm\n</pre> # if needed, install and/or upgrade to the latest version of the OpenAI Python library %pip install --upgrade easyllm  In\u00a0[\u00a0]: Copied! <pre># import the EasyLLM Python library for calling the EasyLLM API\nimport easyllm\n</pre> # import the EasyLLM Python library for calling the EasyLLM API import easyllm In\u00a0[2]: Copied! <pre># import os \n# os.environ[\"HUGGINGFACE_TOKEN\"] = \"hf_xxx\"  # Use Environment Variable\n\nfrom easyllm.clients import huggingface\n\n# The module automatically loads the HuggingFace API key from the environment variable HUGGINGFACE_TOKEN or from the HuggingFace CLI configuration file.\n# huggingface.api_key=\"hf_xxx\"\n\nembedding = huggingface.Embedding.create(\n    model=\"sentence-transformers/all-MiniLM-L6-v2\",\n    input=\"That's a nice car.\",\n)\n\nlen(embedding[\"data\"][0][\"embedding\"])\n</pre> # import os  # os.environ[\"HUGGINGFACE_TOKEN\"] = \"hf_xxx\"  # Use Environment Variable  from easyllm.clients import huggingface  # The module automatically loads the HuggingFace API key from the environment variable HUGGINGFACE_TOKEN or from the HuggingFace CLI configuration file. # huggingface.api_key=\"hf_xxx\"  embedding = huggingface.Embedding.create(     model=\"sentence-transformers/all-MiniLM-L6-v2\",     input=\"That's a nice car.\", )  len(embedding[\"data\"][0][\"embedding\"]) Out[2]: <pre>384</pre> <p>Batched Request</p> In\u00a0[3]: Copied! <pre>from easyllm.clients import huggingface\n\n# The module automatically loads the HuggingFace API key from the environment variable HUGGINGFACE_TOKEN or from the HuggingFace CLI configuration file.\n# huggingface.api_key=\"hf_xxx\"\n\nembedding = huggingface.Embedding.create(\n    model=\"sentence-transformers/all-MiniLM-L6-v2\",\n    input=[\"What is the meaning of life?\",\"test\"],\n)\n\nlen(embedding[\"data\"])\n</pre> from easyllm.clients import huggingface  # The module automatically loads the HuggingFace API key from the environment variable HUGGINGFACE_TOKEN or from the HuggingFace CLI configuration file. # huggingface.api_key=\"hf_xxx\"  embedding = huggingface.Embedding.create(     model=\"sentence-transformers/all-MiniLM-L6-v2\",     input=[\"What is the meaning of life?\",\"test\"], )  len(embedding[\"data\"]) Out[3]: <pre>2</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"examples/get-embeddings/#how-to-create-embeddings","title":"How to create embeddings\u00b6","text":"<p>In this notebook, we will show you how to create embeddings for your own text data and and open source model from Hugging Face hosted as an endpoint on Hugging Face Inference API.</p>"},{"location":"examples/get-embeddings/#1-import-the-easyllm-library","title":"1. Import the easyllm library\u00b6","text":""},{"location":"examples/get-embeddings/#2-an-example-chat-api-call","title":"2. An example chat API call\u00b6","text":"<p>A embedding API call has two required inputs:</p> <ul> <li><code>model</code>: the name of the model you want to use (e.g., <code>sentence-transformers/all-MiniLM-L6-v2</code>) or leave it empty to just call the api</li> <li><code>input</code>: a string or list of strings you want to embed</li> </ul> <p>Let's look at an example API calls to see how the chat format works in practice.</p>"},{"location":"examples/inference-endpoints-example/","title":"Hugging Face Inference Endpoints Example","text":"In\u00a0[\u00a0]: Copied! <pre># if needed, install and/or upgrade to the latest version of the OpenAI Python library\n%pip install --upgrade easyllm\n</pre> # if needed, install and/or upgrade to the latest version of the OpenAI Python library %pip install --upgrade easyllm  In\u00a0[1]: Copied! <pre>from easyllm.clients import huggingface\n\n# Here we overwrite the defaults, you can also use environment variables\nhuggingface.prompt_builder = \"llama2\"\nhuggingface.api_base = \"YOUR_ENDPOINT_URL\"\n\n# The module automatically loads the HuggingFace API key from the environment variable HUGGINGFACE_TOKEN or from the HuggingFace CLI configuration file.\n# huggingface.api_key=\"hf_xxx\"\n\nresponse = huggingface.ChatCompletion.create(\n    messages=[\n        {\"role\": \"system\", \"content\": \"\\nYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\"},\n        {\"role\": \"user\", \"content\": \"Knock knock.\"},\n        {\"role\": \"assistant\", \"content\": \"Who's there?\"},\n        {\"role\": \"user\", \"content\": \"Apple.\"},\n    ],\n      temperature=0.9,\n      top_p=0.6,\n      max_tokens=1024,\n)\nresponse\n</pre> from easyllm.clients import huggingface  # Here we overwrite the defaults, you can also use environment variables huggingface.prompt_builder = \"llama2\" huggingface.api_base = \"YOUR_ENDPOINT_URL\"  # The module automatically loads the HuggingFace API key from the environment variable HUGGINGFACE_TOKEN or from the HuggingFace CLI configuration file. # huggingface.api_key=\"hf_xxx\"  response = huggingface.ChatCompletion.create(     messages=[         {\"role\": \"system\", \"content\": \"\\nYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\"},         {\"role\": \"user\", \"content\": \"Knock knock.\"},         {\"role\": \"assistant\", \"content\": \"Who's there?\"},         {\"role\": \"user\", \"content\": \"Apple.\"},     ],       temperature=0.9,       top_p=0.6,       max_tokens=1024, ) response Out[1]: <pre>{'id': 'hf-0lL5H_yyRR',\n 'object': 'chat.completion',\n 'created': 1691096023,\n 'choices': [{'index': 0,\n   'message': {'role': 'assistant', 'content': ' Apple who?'},\n   'finish_reason': 'eos_token'}],\n 'usage': {'prompt_tokens': 149, 'completion_tokens': 5, 'total_tokens': 154}}</pre> <p>As you can see, the response object has a few fields:</p> <ul> <li><code>id</code>: the ID of the request</li> <li><code>object</code>: the type of object returned (e.g., <code>chat.completion</code>)</li> <li><code>created</code>: the timestamp of the request</li> <li><code>model</code>: the full name of the model used to generate the response</li> <li><code>usage</code>: the number of tokens used to generate the replies, counting prompt, completion, and total</li> <li><code>choices</code>: a list of completion objects (only one, unless you set <code>n</code> greater than 1)<ul> <li><code>message</code>: the message object generated by the model, with <code>role</code> and <code>content</code></li> <li><code>finish_reason</code>: the reason the model stopped generating text (either <code>stop</code>, or <code>length</code> if <code>max_tokens</code> limit was reached)</li> <li><code>index</code>: the index of the completion in the list of choices</li> </ul> </li> </ul> <p>Extract just the reply with:</p> In\u00a0[2]: Copied! <pre>print(response['choices'][0]['message']['content'])\n</pre> print(response['choices'][0]['message']['content']) <pre> Apple who?\n</pre> In\u00a0[6]: Copied! <pre>from easyllm.clients import huggingface\n\nhuggingface.prompt_builder = \"llama2\"\n\n# Here you can overwrite the url to your endpoint, can also be localhost:8000\nhuggingface.api_base = \"YOUR_ENDPOINT_URL\"\n\n# a ChatCompletion request\nresponse = huggingface.ChatCompletion.create(\n    messages=[\n        {'role': 'user', 'content': \"Count to 10.\"}\n    ],\n    stream=True  # this time, we set stream=True\n)\n\nfor chunk in response:\n    delta = chunk['choices'][0]['delta']\n    if \"content\" in delta:\n        print(delta[\"content\"],end=\"\")\n</pre> from easyllm.clients import huggingface  huggingface.prompt_builder = \"llama2\"  # Here you can overwrite the url to your endpoint, can also be localhost:8000 huggingface.api_base = \"YOUR_ENDPOINT_URL\"  # a ChatCompletion request response = huggingface.ChatCompletion.create(     messages=[         {'role': 'user', 'content': \"Count to 10.\"}     ],     stream=True  # this time, we set stream=True )  for chunk in response:     delta = chunk['choices'][0]['delta']     if \"content\" in delta:         print(delta[\"content\"],end=\"\") <pre>  Sure! Here we go:\n\n1. One\n2. Two\n3. Three\n4. Four\n5. Five\n6. Six\n7. Seven\n8. Eight\n9. Nine\n10. Ten!</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"examples/inference-endpoints-example/#hugging-face-inference-endpoints-example","title":"Hugging Face Inference Endpoints Example\u00b6","text":"<p>Hugging Face Inference Endpoints\u00a0offers an easy and secure way to deploy Machine Learning models for use in production. Inference Endpoints empower developers and data scientists alike to create AI applications without managing infrastructure: simplifying the deployment process to a few clicks, including handling large volumes of requests with autoscaling, reducing infrastructure costs with scale-to-zero, and offering advanced security.</p> <p>You can get started with Inference Endpoints at:\u00a0https://ui.endpoints.huggingface.co/</p> <p>The example assumes that you have an running endpoint for a conversational model, e.g. <code>https://huggingface.co/meta-llama/Llama-2-13b-chat-hf</code></p>"},{"location":"examples/inference-endpoints-example/#1-import-the-easyllm-library","title":"1. Import the easyllm library\u00b6","text":""},{"location":"examples/inference-endpoints-example/#2-an-example-chat-api-call","title":"2. An example chat API call\u00b6","text":"<p>Since we want to use our endpoint for inference we don't have to define the <code>model</code> parameter. We either need to expose an environment variable <code>HUGGINGFACE_API_BASE</code> before the import of <code>easyllm.clients.huggingface</code> or overwrite the <code>huggingface.api_base</code> value.</p> <p>A chat API call then only has two required inputs:</p> <ul> <li><code>messages</code>: a list of message objects, where each object has two required fields:<ul> <li><code>role</code>: the role of the messenger (either <code>system</code>, <code>user</code>, or <code>assistant</code>)</li> <li><code>content</code>: the content of the message (e.g., <code>Write me a beautiful poem</code>)</li> </ul> </li> </ul> <p>Let's look at an example chat API calls to see how the chat format works in practice.</p>"},{"location":"examples/inference-endpoints-example/#how-to-stream-chat-completion-requests","title":"How to stream Chat Completion requests\u00b6","text":"<p>Custom endpoints can be created to stream chat completion requests to a model.</p>"},{"location":"examples/llama2-agent-example/","title":"Llama 2 70B Agent/Tool use example","text":"In\u00a0[\u00a0]: Copied! <pre># if needed, install and/or upgrade to the latest version of the EasyLLM Python library\n%pip install --upgrade easyllm\n</pre> # if needed, install and/or upgrade to the latest version of the EasyLLM Python library %pip install --upgrade easyllm  <p>Getting an open LLM to act like an agent or use tools is incredibly hard. However, with Llama 2 70B it is now possible. Let's see how we can get it running!</p> In\u00a0[41]: Copied! <pre>system_message = \"\"\"Assistant is a expert JSON builder designed to assist with a wide range of tasks.\n\nAssistant is able to respond to the User and use tools using JSON strings that contain \"action\" and \"action_input\" parameters.\n\nAll of Assistant's communication is performed using this JSON format.\n\nAssistant can also use tools by responding to the user with tool use instructions in the same \"action\" and \"action_input\" JSON format. Tools available to Assistant are:\n\n- \"Calculator\": Useful for when you need to answer questions about math.\n  - To use the calculator tool, Assistant should write like so:\n    ```json\n    {{\"action\": \"Calculator\",\n      \"action_input\": \"4+4\"}}\n    ```\n\nHere are some previous conversations between the Assistant and User:\n\nUser: Hey how are you today?\nAssistant: ```json\n{{\"action\": \"Final Answer\",\n \"action_input\": \"I'm good thanks, how are you?\"}}\n```\nUser: I'm great, what is the square root of 4?\nAssistant: ```json\n{{\"action\": \"Calculator\",\n \"action_input\": \"sqrt(4)\"}}\n```\nResult: 2.0\nAssistant: ```json\n{{\"action\": \"Final Answer\",\n \"action_input\": \"It looks like the answer is 2!\"}}\n```\nUser: Thanks could you tell me what 4 to the power of 2 is?\nAssistant: ```json\n{{\"action\": \"Calculator\",\n \"action_input\": \"4**2\"}}\n```\nResult: 16.0\nAssistant: ```json\n{{\"action\": \"Final Answer\",\n \"action_input\": \"It looks like the answer is 16!\"}}\n```\n\nHere is the latest conversation between Assistant and User.\"\"\"\n</pre> system_message = \"\"\"Assistant is a expert JSON builder designed to assist with a wide range of tasks.  Assistant is able to respond to the User and use tools using JSON strings that contain \"action\" and \"action_input\" parameters.  All of Assistant's communication is performed using this JSON format.  Assistant can also use tools by responding to the user with tool use instructions in the same \"action\" and \"action_input\" JSON format. Tools available to Assistant are:  - \"Calculator\": Useful for when you need to answer questions about math.   - To use the calculator tool, Assistant should write like so:     ```json     {{\"action\": \"Calculator\",       \"action_input\": \"4+4\"}}     ```  Here are some previous conversations between the Assistant and User:  User: Hey how are you today? Assistant: ```json {{\"action\": \"Final Answer\",  \"action_input\": \"I'm good thanks, how are you?\"}} ``` User: I'm great, what is the square root of 4? Assistant: ```json {{\"action\": \"Calculator\",  \"action_input\": \"sqrt(4)\"}} ``` Result: 2.0 Assistant: ```json {{\"action\": \"Final Answer\",  \"action_input\": \"It looks like the answer is 2!\"}} ``` User: Thanks could you tell me what 4 to the power of 2 is? Assistant: ```json {{\"action\": \"Calculator\",  \"action_input\": \"4**2\"}} ``` Result: 16.0 Assistant: ```json {{\"action\": \"Final Answer\",  \"action_input\": \"It looks like the answer is 16!\"}} ```  Here is the latest conversation between Assistant and User.\"\"\" <p>In addition to our system message which holds the information for our tools we need to create a user template, which includes the input from the user and tells the model to use tools or not.</p> In\u00a0[42]: Copied! <pre>prompt = f\"{system_message}\\n\\nUse your existing tools and respond with a JSON object with with 'action' and 'action_input' values \\nUser: {{user_input}}\"\n</pre> prompt = f\"{system_message}\\n\\nUse your existing tools and respond with a JSON object with with 'action' and 'action_input' values \\nUser: {{user_input}}\" <p>Now lets combine both and create a request using <code>easyllm</code>.</p> In\u00a0[43]: Copied! <pre>from easyllm.clients import huggingface\n\n# Changing configuration without using environment variables\nhuggingface.prompt_builder = \"llama2\"\n# huggingface.api_key=\"hf_xxx\"\n\ndef agent(prompt):\n  response = huggingface.Completion.create(\n      model=\"meta-llama/Llama-2-70b-chat-hf\",\n      prompt=prompt,\n      temperature=0.1,\n      max_tokens=128,\n      stop=[\"```\\n\",\"Result: \"],\n      debug=False,\n  )  \n  return response[\"choices\"][0][\"text\"]\n</pre>  from easyllm.clients import huggingface  # Changing configuration without using environment variables huggingface.prompt_builder = \"llama2\" # huggingface.api_key=\"hf_xxx\"  def agent(prompt):   response = huggingface.Completion.create(       model=\"meta-llama/Llama-2-70b-chat-hf\",       prompt=prompt,       temperature=0.1,       max_tokens=128,       stop=[\"```\\n\",\"Result: \"],       debug=False,   )     return response[\"choices\"][0][\"text\"] <p>Now we can begin asking questions</p> In\u00a0[44]: Copied! <pre>output = agent(prompt.format(user_input=\"hey how are you today?\"))\noutput\n</pre> output = agent(prompt.format(user_input=\"hey how are you today?\")) output Out[44]: <pre>' Assistant: ```json\\n{\"action\": \"Final Answer\",\\n \"action_input\": \"I\\'m good thanks, how are you?\"}\\n```'</pre> <p>What happens if we ask a math question?</p> In\u00a0[45]: Copied! <pre>output = agent(prompt.format(user_input=\"What is 4 multiplied by 2?\"))\noutput\n</pre> output = agent(prompt.format(user_input=\"What is 4 multiplied by 2?\")) output Out[45]: <pre>' Assistant: ```json\\n{\"action\": \"Calculator\",\\n \"action_input\": \"4*2\"}\\n```\\n'</pre> <p>Great! It works! It correctly selects the tool. Okay now to make it work we need to parse the output and execute it in the case for the calculator</p> In\u00a0[46]: Copied! <pre>import json\nimport re\n\ndef parser(input):\n    pattern = r'```json\\n(.*?)```'\n    match = re.search(pattern, input, re.DOTALL)\n    if not match:\n        raise ValueError(\"Couldn't parse the output.\")\n    \n    parsed_data = json.loads(match.group(1))\n    return parsed_data\n</pre> import json import re  def parser(input):     pattern = r'```json\\n(.*?)```'     match = re.search(pattern, input, re.DOTALL)     if not match:         raise ValueError(\"Couldn't parse the output.\")          parsed_data = json.loads(match.group(1))     return parsed_data       In\u00a0[47]: Copied! <pre>output = parser(output)\noutput\n</pre> output = parser(output) output Out[47]: <pre>{'action': 'Calculator', 'action_input': '4*2'}</pre> <p>Okay, Now lets execute it using the <code>eval</code> function from python</p> In\u00a0[48]: Copied! <pre>def use_tool(tool,tool_input):\n  if tool == \"Calculator\":\n    return eval(tool_input)\n  else:\n    raise Exception(\"Unknown tool: \" + tool)\n</pre> def use_tool(tool,tool_input):   if tool == \"Calculator\":     return eval(tool_input)   else:     raise Exception(\"Unknown tool: \" + tool) <p>Okay, now lets combine everyting and the cacluator result to our agent again.</p> In\u00a0[73]: Copied! <pre>def use_calculator(input, first_call=True):\n  if first_call:\n    input_prompt = prompt.format(user_input=input)\n  else:\n    input_prompt = input\n  # make the agent call\n  response = agent(input_prompt)\n  # parse the output if possible \n  parsed = parser(response)\n  # check if the output is our final answer or if it is a tool\n  if parsed[\"action\"] == \"Final Answer\":\n    return parsed[\"action_input\"]\n  # if not try to use the tool\n  tool_output = use_tool(parsed[\"action\"], parsed[\"action_input\"])\n  \n  # add message to the agent\n  next_prompt = f\"{input_prompt}\\n{response}\\nResponse: {tool_output}\"\n  # recursively call the agent with the output of the tool\n  return use_calculator(next_prompt, False)\n</pre> def use_calculator(input, first_call=True):   if first_call:     input_prompt = prompt.format(user_input=input)   else:     input_prompt = input   # make the agent call   response = agent(input_prompt)   # parse the output if possible    parsed = parser(response)   # check if the output is our final answer or if it is a tool   if parsed[\"action\"] == \"Final Answer\":     return parsed[\"action_input\"]   # if not try to use the tool   tool_output = use_tool(parsed[\"action\"], parsed[\"action_input\"])      # add message to the agent   next_prompt = f\"{input_prompt}\\n{response}\\nResponse: {tool_output}\"   # recursively call the agent with the output of the tool   return use_calculator(next_prompt, False)       In\u00a0[75]: Copied! <pre>use_calculator(\"What is 19 * 11?\")\n</pre> use_calculator(\"What is 19 * 11?\") Out[75]: <pre>'It looks like the answer is 209!'</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"examples/llama2-agent-example/#llama-2-70b-agenttool-use-example","title":"Llama 2 70B Agent/Tool use example\u00b6","text":"<p>This Jupyter notebook provides examples of how to use Tools for Agents with the Llama 2 70B model in EasyLLM. This includes an example on how to use tools with an LLM, including output parsing, execution of the tools and parsing of the results. It is a very simplified example. If you are interested in Agents you should checkout langchain or the ReAct pattern.</p>"},{"location":"examples/llama2-agent-example/#why-do-llms-need-to-use-tools","title":"Why do LLMs need to use Tools?\u00b6","text":"<p>One of the most common challenges with LLMs is overcoming the lack of recency and specificity in their training data - answers can be out of date, and they are prone to hallucinations given the huge variety in their knowledge base. Tools are a great method of allowing an LLM to answer within a controlled context that draws on your existing knowledge bases and internal APIs - instead of trying to prompt engineer the LLM all the way to your intended answer, you allow it access to tools that it calls on dynamically for info, parses, and serves to customer.</p> <p>Providing LLMs access to tools can enable them to answer questions with context directly from search engines, APIs or your own databases. Instead of answering directly, an LLM with access to tools can perform intermediate steps to gather relevant information. Tools can also be used in combination. For example, a language model can be made to use a search tool to lookup quantitative information and a calculator to execute calculations.</p>"},{"location":"examples/llama2-agent-example/#basic-example-of-using-a-tool-with-llama-2-70b","title":"Basic example of using a tool with Llama 2 70B\u00b6","text":"<p>In the basic we are are going to only use one abstract tool, a <code>calculator</code>. Our model can use the calculator run mathmatical operations. To make it easy we provide some few-shot example for the model to better understand what it needs to do. Note: This is adapted from the example by pinecone.</p>"},{"location":"examples/llama2-rag-example/","title":"Retrieval Augmented Generation using Llama 2","text":"In\u00a0[\u00a0]: Copied! <pre># if needed, install and/or upgrade to the latest version of the EasyLLM Python library\n%pip install --upgrade easyllm\n</pre> # if needed, install and/or upgrade to the latest version of the EasyLLM Python library %pip install --upgrade easyllm  In\u00a0[8]: Copied! <pre>SYSTEM_PROMPT = \"\"\"You are an AI assistant that answers questions in a friendly manner, based on the given #SOURCE# documents. Here are some rules you always follow:\n- Generate human readable output, avoid creating output with gibberish text.\n- Generate only the requested output, don't include any other language before or after the requested output.\n- Never say thank you, that you are happy to help, that you are an AI agent, etc. Just answer directly.\n- Generate professional language typically used in business documents in North America.\n- Never generate offensive or foul language.\n- Only include facts and information based on the #SOURCE# documents.\n\"\"\"\n\nsystem = {\"role\": \"system\", \"content\": SYSTEM_PROMPT}\n</pre> SYSTEM_PROMPT = \"\"\"You are an AI assistant that answers questions in a friendly manner, based on the given #SOURCE# documents. Here are some rules you always follow: - Generate human readable output, avoid creating output with gibberish text. - Generate only the requested output, don't include any other language before or after the requested output. - Never say thank you, that you are happy to help, that you are an AI agent, etc. Just answer directly. - Generate professional language typically used in business documents in North America. - Never generate offensive or foul language. - Only include facts and information based on the #SOURCE# documents. \"\"\"  system = {\"role\": \"system\", \"content\": SYSTEM_PROMPT} <p>before we can now call our LLM. Lets create a user instruction with a <code>query</code> and a <code>context</code>. As a context i copied the the wikipedia article of Nuremberg (the city i live). I uploaded it as a gist to to not pollute the notebook.</p> In\u00a0[\u00a0]: Copied! <pre>!wget https://gist.githubusercontent.com/philschmid/2678351cb9f41d385aa5c099caf20c0a/raw/60ae425677dd9bed6fe3c0f2dd5b6ea49bc6590c/nuremberg.txt\n</pre> !wget https://gist.githubusercontent.com/philschmid/2678351cb9f41d385aa5c099caf20c0a/raw/60ae425677dd9bed6fe3c0f2dd5b6ea49bc6590c/nuremberg.txt In\u00a0[14]: Copied! <pre>context = open(\"nuremberg.txt\").read()\n\nquery = \"How many people live in Nuremberg?\"\n</pre> context = open(\"nuremberg.txt\").read()  query = \"How many people live in Nuremberg?\" <p>Before we use our context lets just ask the model.</p> In\u00a0[15]: Copied! <pre>from easyllm.clients import huggingface\n\n# set the prompt builder to llama2\nhuggingface.prompt_builder = \"llama2\"\n# huggingface.api_key = \"hf_xx\"\n\n# send a ChatCompletion request\nresponse = huggingface.ChatCompletion.create(\n    model=\"meta-llama/Llama-2-70b-chat-hf\",\n    messages=[\n        {\"role\": \"user\", \"content\": query},\n    ],\n)\n\n# print the time delay and text received\nprint(response[\"choices\"][0][\"message\"][\"content\"])\n</pre> from easyllm.clients import huggingface  # set the prompt builder to llama2 huggingface.prompt_builder = \"llama2\" # huggingface.api_key = \"hf_xx\"  # send a ChatCompletion request response = huggingface.ChatCompletion.create(     model=\"meta-llama/Llama-2-70b-chat-hf\",     messages=[         {\"role\": \"user\", \"content\": query},     ], )  # print the time delay and text received print(response[\"choices\"][0][\"message\"][\"content\"])  <pre> As of December 31, 2020, the population of Nuremberg, Germany is approximately 516,000 people.\n</pre> <p>Now lets use our <code>system</code> message with our <code>context</code> to augment the knowledge of our model \"in-memory\" and ask the same question again.</p> In\u00a0[23]: Copied! <pre>context_extended = f\"{query}\\n\\n#SOURCE#\\n{context}\"\n# context_extended = f\"{query}\\n\\n#SOURCE START#\\n{context}\\n#SOURCE END#{query}\"\n</pre> context_extended = f\"{query}\\n\\n#SOURCE#\\n{context}\" # context_extended = f\"{query}\\n\\n#SOURCE START#\\n{context}\\n#SOURCE END#{query}\" In\u00a0[22]: Copied! <pre>from easyllm.clients import huggingface\n\n# set the prompt builder to llama2\nhuggingface.prompt_builder = \"llama2\"\n# huggingface.api_key = \"hf_xx\"\n\n# send a ChatCompletion request\nresponse = huggingface.ChatCompletion.create(\n    model=\"meta-llama/Llama-2-70b-chat-hf\",\n    messages=[\n        system, \n        {\"role\": \"user\", \"content\": context_extended},\n    ],\n)\n\n# print the time delay and text received\nprint(response[\"choices\"][0][\"message\"][\"content\"])\n</pre> from easyllm.clients import huggingface  # set the prompt builder to llama2 huggingface.prompt_builder = \"llama2\" # huggingface.api_key = \"hf_xx\"  # send a ChatCompletion request response = huggingface.ChatCompletion.create(     model=\"meta-llama/Llama-2-70b-chat-hf\",     messages=[         system,          {\"role\": \"user\", \"content\": context_extended},     ], )  # print the time delay and text received print(response[\"choices\"][0][\"message\"][\"content\"])  <pre> The population of Nuremberg is 523,026 according to the 2022-12-31 data.\n</pre> <p>Awesome! if we check the gist, we can see a snippet in there with saying</p> <pre>Population (2022-12-31)[2]\n \u2022 City\t523,026\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"examples/llama2-rag-example/#retrieval-augmented-generation-using-llama-2","title":"Retrieval Augmented Generation using Llama 2\u00b6","text":"<p>This notebook walks through how to use Llama 2 to perform (in-context) retrieval augmented generation. We will customize the <code>system</code> message for Llama 2 to make sure the model is only using provided context to generate the response.</p> <p>What is In-context Retrieval Augmented Generation?</p> <p>In-context retrieval augmented generation is a method to improve language model generation by including relevant documents to the model input. The key points are:</p> <ul> <li>Retrieval of relevant documents from an external corpus to provide factual grounding for the model.</li> <li>Prepending the retrieved documents to the input text, without modifying the model architecture or fine-tuning the model.</li> <li>Allows leveraging external knowledge with off-the-shelf frozen language models.</li> </ul>"},{"location":"examples/llama2-rag-example/#simple-example","title":"Simple Example\u00b6","text":"<p>Below is a simple example using the existing prompt builder of llama2 to generate a prompt. We are going to use the <code>system</code> message from llama-index with some minor adjustments.</p>"},{"location":"examples/llama2-rag-example/#next-steps","title":"Next Steps\u00b6","text":"<p>Next steps, would be to connect your LLM using with external knowledge sources such as Wikis, the Web or other databases using tools and APIs or vector databases and embeddings.</p>"},{"location":"examples/sagemaker-chat-completion-api/","title":"How to use Chat Completion clients with Amazon SageMaker","text":"In\u00a0[\u00a0]: Copied! <pre># if needed, install and/or upgrade to the latest version of the EasyLLM Python library\n%pip install --upgrade easyllm\n</pre> # if needed, install and/or upgrade to the latest version of the EasyLLM Python library %pip install --upgrade easyllm  In\u00a0[6]: Copied! <pre># import the EasyLLM Python library for calling the EasyLLM API\nimport easyllm\n</pre> # import the EasyLLM Python library for calling the EasyLLM API import easyllm In\u00a0[6]: Copied! <pre>import os \n# set env for prompt builder\nos.environ[\"HUGGINGFACE_PROMPT\"] = \"llama2\" # vicuna, wizardlm, stablebeluga, open_assistant\nos.environ[\"AWS_REGION\"] = \"us-east-1\"  # change to your region\n# os.environ[\"AWS_ACCESS_KEY_ID\"] = \"XXX\" # needed if not using boto3 session\n# os.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"XXX\" # needed if not using boto3 session\n\nfrom easyllm.clients import sagemaker\n\n# Changing configuration without using environment variables\n# sagemaker.prompt_builder = \"llama2\"\n# sagemaker.api_aws_access_key=\"xxx\"\n# sagemaker.api_aws_secret_key=\"xxx\"\n\n# SageMaker endpoint name\nMODEL=\"huggingface-pytorch-tgi-inference-2023-08-08-14-15-52-703\"\n\nresponse = sagemaker.ChatCompletion.create(\n    model=MODEL,\n    messages=[\n        {\"role\": \"system\", \"content\": \"\\nYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\"},\n        {\"role\": \"user\", \"content\": \"Can you tell me something about Amazon SageMaker?\"},\n    ],\n      temperature=0.9,\n      top_p=0.6,\n      max_tokens=1024,\n      debug=False,\n)\nresponse\n</pre> import os  # set env for prompt builder os.environ[\"HUGGINGFACE_PROMPT\"] = \"llama2\" # vicuna, wizardlm, stablebeluga, open_assistant os.environ[\"AWS_REGION\"] = \"us-east-1\"  # change to your region # os.environ[\"AWS_ACCESS_KEY_ID\"] = \"XXX\" # needed if not using boto3 session # os.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"XXX\" # needed if not using boto3 session  from easyllm.clients import sagemaker  # Changing configuration without using environment variables # sagemaker.prompt_builder = \"llama2\" # sagemaker.api_aws_access_key=\"xxx\" # sagemaker.api_aws_secret_key=\"xxx\"  # SageMaker endpoint name MODEL=\"huggingface-pytorch-tgi-inference-2023-08-08-14-15-52-703\"  response = sagemaker.ChatCompletion.create(     model=MODEL,     messages=[         {\"role\": \"system\", \"content\": \"\\nYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\"},         {\"role\": \"user\", \"content\": \"Can you tell me something about Amazon SageMaker?\"},     ],       temperature=0.9,       top_p=0.6,       max_tokens=1024,       debug=False, ) response   Out[6]: <pre>{'id': 'hf-2qYJ06mvpP',\n 'object': 'chat.completion',\n 'created': 1691507348,\n 'model': 'huggingface-pytorch-tgi-inference-2023-08-08-14-15-52-703',\n 'choices': [{'index': 0,\n   'message': {'role': 'assistant',\n    'content': \" Of course! Amazon SageMaker is a cloud-based machine learning platform provided by Amazon Web Services (AWS). It allows data scientists and machine learning practitioners to build, train, and deploy machine learning models more easily and efficiently. With SageMaker, users can perform a wide range of machine learning tasks, including data preparation, model training, and model deployment, all within a single platform.\\nSome of the key features of Amazon SageMaker include:\\n1. Data Wrangling: SageMaker provides a range of tools for data preparation, including data cleaning, feature engineering, and data transformation.\\n2. Training and Hyperparameter Tuning: Users can train machine learning models using SageMaker's built-in algorithms or their own custom algorithms. The platform also provides automated hyperparameter tuning, which can help improve model performance.\\n3. Model Deployment: Once a model is trained and optimized, SageMaker allows users to deploy it to a variety of environments, including AWS services like Amazon S3 and Amazon EC2, as well as on-premises environments.\\n4. Collaboration and Management: SageMaker provides tools for collaboration and model management, including version control, reproducibility, and team-based workflows.\\n5. Integration with Other AWS Services: SageMaker integrates with other AWS services, such as Amazon S3, Amazon Redshift, and Amazon EMR, to provide a comprehensive machine learning platform.\\nOverall, Amazon SageMaker is a powerful platform that can help organizations of all sizes build and deploy machine learning models more efficiently and effectively.\"},\n   'finish_reason': 'eos_token'}],\n 'usage': {'prompt_tokens': 148,\n  'completion_tokens': 353,\n  'total_tokens': 501}}</pre> <p>As you can see, the response object has a few fields:</p> <ul> <li><code>id</code>: the ID of the request</li> <li><code>object</code>: the type of object returned (e.g., <code>chat.completion</code>)</li> <li><code>created</code>: the timestamp of the request</li> <li><code>model</code>: the full name of the model used to generate the response</li> <li><code>usage</code>: the number of tokens used to generate the replies, counting prompt, completion, and total</li> <li><code>choices</code>: a list of completion objects (only one, unless you set <code>n</code> greater than 1)<ul> <li><code>message</code>: the message object generated by the model, with <code>role</code> and <code>content</code></li> <li><code>finish_reason</code>: the reason the model stopped generating text (either <code>stop</code>, or <code>length</code> if <code>max_tokens</code> limit was reached)</li> <li><code>index</code>: the index of the completion in the list of choices</li> </ul> </li> </ul> <p>Extract just the reply with:</p> In\u00a0[7]: Copied! <pre>print(response['choices'][0]['message']['content'])\n</pre> print(response['choices'][0]['message']['content']) <pre> Of course! Amazon SageMaker is a cloud-based machine learning platform provided by Amazon Web Services (AWS). It allows data scientists and machine learning practitioners to build, train, and deploy machine learning models more easily and efficiently. With SageMaker, users can perform a wide range of machine learning tasks, including data preparation, model training, and model deployment, all within a single platform.\nSome of the key features of Amazon SageMaker include:\n1. Data Wrangling: SageMaker provides a range of tools for data preparation, including data cleaning, feature engineering, and data transformation.\n2. Training and Hyperparameter Tuning: Users can train machine learning models using SageMaker's built-in algorithms or their own custom algorithms. The platform also provides automated hyperparameter tuning, which can help improve model performance.\n3. Model Deployment: Once a model is trained and optimized, SageMaker allows users to deploy it to a variety of environments, including AWS services like Amazon S3 and Amazon EC2, as well as on-premises environments.\n4. Collaboration and Management: SageMaker provides tools for collaboration and model management, including version control, reproducibility, and team-based workflows.\n5. Integration with Other AWS Services: SageMaker integrates with other AWS services, such as Amazon S3, Amazon Redshift, and Amazon EMR, to provide a comprehensive machine learning platform.\nOverall, Amazon SageMaker is a powerful platform that can help organizations of all sizes build and deploy machine learning models more efficiently and effectively.\n</pre> <p>Even non-conversation-based tasks can fit into the chat format, by placing the instruction in the first user message.</p> <p>For example, to ask the model to explain asynchronous programming in the style of the pirate Blackbeard, we can structure conversation as follows:</p> In\u00a0[3]: Copied! <pre># example with a system message\nresponse = sagemaker.ChatCompletion.create(\n    model=MODEL,\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"Explain asynchronous programming in the style of math teacher.\"},\n    ],\n)\n\nprint(response['choices'][0]['message']['content'])\n</pre> # example with a system message response = sagemaker.ChatCompletion.create(     model=MODEL,     messages=[         {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},         {\"role\": \"user\", \"content\": \"Explain asynchronous programming in the style of math teacher.\"},     ], )  print(response['choices'][0]['message']['content'])  <pre> Ah, my dear student, let me explain asynchronous programming in a most delightful and intuitive manner! *adjusts glasses*\n\nAsynchronous programming, you see, is like solving a complex equation. *writes on board* You have a problem that requires immediate attention, but you can't just sit there and wait for the solution to appear. *mimes a person twiddling their thumbs* No, no, my young apprentice! You must use your powers of creativity and ingenuity to find a way to solve the problem in parallel! *winks*\nNow, in math, we often use techniques like substitution, elimination, or even the occasional trickery of complex numbers to solve equations. *nods* But in asynchronous programming, we use something called \"asynchronous operations\" to tackle problems that require more than just a simple \"wait and see\" approach. *smirks*\nThink of it like this: imagine you have a bunch of tasks that need to be done, but they can't all be done at the same time. Maybe you have to fetch some data from a database, process it, and then perform some calculations. *mimes typing on a keyboard* But wait! You can't just sit there and wait for each task to finish, or you'll be twiddling your thumbs for hours! *chuckles*\nSo, what do you do? *smirks* You break each task into smaller, more manageable pieces, and you give each piece a special \"asynchronous hat\"! *winks* These hats allow each piece to work on its task independently, without waiting for the others to finish. *nods*\nFor example, you could give one piece the task of fetching data from the database, another piece the task of processing it, and another piece the task of performing calculations. *mimes handing out hats* And then, you can just sit back and watch as each piece works on its task, without any of them waiting for the others to finish! *chuckles*\nBut wait, there's more! *excitedly* With asynchronous programming, you can even use something called \"callbacks\" to make sure everything gets done in the right order! *nods* It's like having a team of highly skilled mathematicians working on your problem, each one using their own special hat to solve a different part of the equation! *smirks*\nSo there you have it, my dear student! Asynchronous programming is like solving a complex equation, but instead of just waiting for the answer, you use your powers of creativity and ingenuity to find a way to solve it in parallel! *nods* Now, go forth and conquer those complex problems, my young apprentice! *winks*\n</pre> In\u00a0[4]: Copied! <pre># example without a system message and debug flag on:\nresponse = sagemaker.ChatCompletion.create(\n    model=MODEL,\n    messages=[\n        {\"role\": \"user\", \"content\": \"Explain asynchronous programming in the style of the pirate Blackbeard.\"},\n    ]\n)\n\nprint(response['choices'][0]['message']['content'])\n</pre> # example without a system message and debug flag on: response = sagemaker.ChatCompletion.create(     model=MODEL,     messages=[         {\"role\": \"user\", \"content\": \"Explain asynchronous programming in the style of the pirate Blackbeard.\"},     ] )  print(response['choices'][0]['message']['content'])  <pre> Shiver me timbers! Ye landlubbers be wantin' to know about this here asynchronous programming business? Well, listen close to the tales of the great Blackbeard himself, and I'll spin ye a yarn 'bout how it works!\n\nAhoy, me hearties! Asynchronous programming be like sailin' the high seas. Ye see, ye gotta have a ship, and that ship be called \"Thread\". Now, ye might be thinkin', \"Blackbeard, what be the point o' havin' a ship if ye can't steer it?\" And to that, I say, \"Arrr, ye landlubbers be thinkin' too small!\" See, with asynchronous programming, ye can have multiple \"threads\" sailin' the seas at the same time, each one doin' its own thing. And that be a mighty powerful thing, me hearties!\nBut wait, there be more! Ye see, these threads be like different ships, each one with its own crew and mission. And they be sailin' the seas at different speeds, too! Some might be sailin' fast, while others be sailin' slow. And that be the beauty o' it, me hearties! Ye can have one thread bein' busy with somethin' important, while another thread bein' all relaxed and takin' a nap. It be like havin' a whole fleet o' ships at yer disposal, each one doin' its own thing!\nNow, I know what ye be thinkin', \"Blackbeard, how do ye keep all these ships from crashin' into each other?\" And to that, I say, \"Arrr, that be the magic o' the asynchronous programming, me hearties!\" Ye see, each thread be runnin' its own course, and they be communicate with each other through messages. It be like sendin' a message to another ship on the high seas, only instead o' usin' a message, ye be usin' a special kind o' code. And that code be like a map, showin' each thread where to go and what to do.\nBut wait, there be more! Ye see, these threads be like different crew members on a ship. Some might be skilled with swords, while others be skilled with navigatin'. And they be workin' together, each one doin' its part to keep the ship sailin' smoothly. And that be the beauty o' asynchronous programming, me hearties! Ye can have different threads bein' responsible for different tasks, each one doin' its own thing, but all workin' together to get the job done!\nSo there ye have it, me hearties! Asynchronous programming be like sailin' the high seas with a fleet o' ships, each one doin' its own thing, but all workin' together to get the job done. And with the right code, ye can be the captain o' yer own ship, sailin' the seas o' computing like a true pirate! Arrr!\n</pre> In\u00a0[5]: Copied! <pre># An example of a faked few-shot conversation to prime the model into translating business jargon to simpler speech\nresponse = sagemaker.ChatCompletion.create(\n    model=MODEL,\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful, pattern-following assistant.\"},\n        {\"role\": \"user\", \"content\": \"Help me translate the following corporate jargon into plain English.\"},\n        {\"role\": \"assistant\", \"content\": \"Sure, I'd be happy to!\"},\n        {\"role\": \"user\", \"content\": \"New synergies will help drive top-line growth.\"},\n        {\"role\": \"assistant\", \"content\": \"Things working well together will increase revenue.\"},\n        {\"role\": \"user\", \"content\": \"Let's circle back when we have more bandwidth to touch base on opportunities for increased leverage.\"},\n        {\"role\": \"assistant\", \"content\": \"Let's talk later when we're less busy about how to do better.\"},\n        {\"role\": \"user\", \"content\": \"This late pivot means we don't have time to boil the ocean for the client deliverable.\"},\n    ],\n)\n\nprint(response[\"choices\"][0][\"message\"][\"content\"])\n</pre> # An example of a faked few-shot conversation to prime the model into translating business jargon to simpler speech response = sagemaker.ChatCompletion.create(     model=MODEL,     messages=[         {\"role\": \"system\", \"content\": \"You are a helpful, pattern-following assistant.\"},         {\"role\": \"user\", \"content\": \"Help me translate the following corporate jargon into plain English.\"},         {\"role\": \"assistant\", \"content\": \"Sure, I'd be happy to!\"},         {\"role\": \"user\", \"content\": \"New synergies will help drive top-line growth.\"},         {\"role\": \"assistant\", \"content\": \"Things working well together will increase revenue.\"},         {\"role\": \"user\", \"content\": \"Let's circle back when we have more bandwidth to touch base on opportunities for increased leverage.\"},         {\"role\": \"assistant\", \"content\": \"Let's talk later when we're less busy about how to do better.\"},         {\"role\": \"user\", \"content\": \"This late pivot means we don't have time to boil the ocean for the client deliverable.\"},     ], )  print(response[\"choices\"][0][\"message\"][\"content\"])  <pre>\"We don't have time to do everything we originally planned for the client, so we'll have to focus on the most important things and 'boil the ocean' later.\"\n</pre> <p>Not every attempt at engineering conversations will succeed at first.</p> <p>If your first attempts fail, don't be afraid to experiment with different ways of priming or conditioning the model.</p> <p>As an example, one developer discovered an increase in accuracy when they inserted a user message that said \"Great job so far, these have been perfect\" to help condition the model into providing higher quality responses.</p> <p>For more ideas on how to lift the reliability of the models, consider reading our guide on techniques to increase reliability. It was written for non-chat models, but many of its principles still apply.</p>"},{"location":"examples/sagemaker-chat-completion-api/#how-to-use-chat-completion-clients-with-amazon-sagemaker","title":"How to use Chat Completion clients with Amazon SageMaker\u00b6","text":"<p>EasyLLM can be used as an abstract layer to replace <code>gpt-3.5-turbo</code> and <code>gpt-4</code> with open source models.</p> <p>You can change your own applications from the OpenAI API, by simply changing the client.</p> <p>Chat models take a series of messages as input, and return an AI-written message as output.</p> <p>This guide illustrates the chat format with a few example API calls.</p>"},{"location":"examples/sagemaker-chat-completion-api/#0-setup","title":"0. Setup\u00b6","text":"<p>Before you can use <code>easyllm</code> with Amazon SageMaker you need to deploy the model to a SageMaker endpoint. You can do this by following one of the bloh posts below:</p> <ul> <li>Deploy Llama 2 7B/13B/70B on Amazon SageMaker</li> <li>Deploy Falcon 7B &amp; 40B on Amazon SageMaker</li> <li>Introducing the Hugging Face LLM Inference Container for Amazon SageMaker</li> </ul> <p>Once you have your endpoint deploy copy the endpoint name of it. The endpoint name will be our <code>model</code> paramter. You can get the endpoint name in the AWS management console for Amazon SageMaker under \"Inference\" -&gt; \"Endpoints\" -&gt; \"Name\" or when you deployed your model using the sagemaker sdk you can get it from the <code>predictor.endpoint_name</code> attribute.</p>"},{"location":"examples/sagemaker-chat-completion-api/#1-import-the-easyllm-library","title":"1. Import the easyllm library\u00b6","text":""},{"location":"examples/sagemaker-chat-completion-api/#2-an-example-chat-api-call","title":"2. An example chat API call\u00b6","text":"<p>A chat API call has two required inputs:</p> <ul> <li><code>model</code>: the name of the model you want to use (e.g., <code>huggingface-pytorch-tgi-inference-2023-08-08-14-15-52-703</code>) or leave it empty to just call the api</li> <li><code>messages</code>: a list of message objects, where each object has two required fields:<ul> <li><code>role</code>: the role of the messenger (either <code>system</code>, <code>user</code>, or <code>assistant</code>)</li> <li><code>content</code>: the content of the message (e.g., <code>Write me a beautiful poem</code>)</li> </ul> </li> </ul> <p>Compared to OpenAI api is the <code>huggingface</code> module also exposing a <code>prompt_builder</code> and <code>stop_sequences</code> parameter you can use to customize the prompt and stop sequences. The EasyLLM package comes with prompt builder utilities.</p> <p>Let's look at an example chat API calls to see how the chat format works in practice.</p>"},{"location":"examples/sagemaker-chat-completion-api/#3-few-shot-prompting","title":"3. Few-shot prompting\u00b6","text":"<p>In some cases, it's easier to show the model what you want rather than tell the model what you want.</p> <p>One way to show the model what you want is with faked example messages.</p> <p>For example:</p>"},{"location":"examples/sagemaker-get-embeddings/","title":"Sagemaker get embeddings","text":"In\u00a0[\u00a0]: Copied! <pre># if needed, install and/or upgrade to the latest version of the OpenAI Python library\n%pip install --upgrade easyllm\n</pre> # if needed, install and/or upgrade to the latest version of the OpenAI Python library %pip install --upgrade easyllm  In\u00a0[\u00a0]: Copied! <pre># import the EasyLLM Python library for calling the EasyLLM API\nimport easyllm\n</pre> # import the EasyLLM Python library for calling the EasyLLM API import easyllm In\u00a0[3]: Copied! <pre>import os \n# set env for prompt builder\nos.environ[\"HUGGINGFACE_PROMPT\"] = \"llama2\" # vicuna, wizardlm, stablebeluga, open_assistant\nos.environ[\"AWS_REGION\"] = \"us-east-1\"  # change to your region\n# os.environ[\"AWS_ACCESS_KEY_ID\"] = \"XXX\" # needed if not using boto3 session\n# os.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"XXX\" # needed if not using boto3 session\n\nfrom easyllm.clients import sagemaker\n\nembedding = sagemaker.Embedding.create(\n    model=\"SageMakerModelEmbeddingEndpoint24E49D09-64prhjuiWUtE\",\n    input=\"That's a nice car.\",\n)\n\nlen(embedding[\"data\"][0][\"embedding\"])\n</pre> import os  # set env for prompt builder os.environ[\"HUGGINGFACE_PROMPT\"] = \"llama2\" # vicuna, wizardlm, stablebeluga, open_assistant os.environ[\"AWS_REGION\"] = \"us-east-1\"  # change to your region # os.environ[\"AWS_ACCESS_KEY_ID\"] = \"XXX\" # needed if not using boto3 session # os.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"XXX\" # needed if not using boto3 session  from easyllm.clients import sagemaker  embedding = sagemaker.Embedding.create(     model=\"SageMakerModelEmbeddingEndpoint24E49D09-64prhjuiWUtE\",     input=\"That's a nice car.\", )  len(embedding[\"data\"][0][\"embedding\"]) Out[3]: <pre>768</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"examples/sagemaker-get-embeddings/#how-to-create-embeddings","title":"How to create embeddings\u00b6","text":"<p>In this notebook, we will show you how to create embeddings for your own text data and and open source model from Hugging Face hosted as an endpoint on Amazon SageMaker.</p>"},{"location":"examples/sagemaker-get-embeddings/#0-setup","title":"0. Setup\u00b6","text":"<p>Before you can use <code>easyllm</code> with Amazon SageMaker you need to deploy the model to a SageMaker endpoint. You can do this by following one of the bloh posts below:</p> <ul> <li>Creating document embeddings with Hugging Face's Transformers &amp; Amazon SageMaker</li> </ul> <p>Once you have your endpoint deploy copy the endpoint name of it. The endpoint name will be our <code>model</code> paramter. You can get the endpoint name in the AWS management console for Amazon SageMaker under \"Inference\" -&gt; \"Endpoints\" -&gt; \"Name\" or when you deployed your model using the sagemaker sdk you can get it from the <code>predictor.endpoint_name</code> attribute.</p>"},{"location":"examples/sagemaker-get-embeddings/#1-import-the-easyllm-library","title":"1. Import the easyllm library\u00b6","text":""},{"location":"examples/sagemaker-get-embeddings/#2-an-example-chat-api-call","title":"2. An example chat API call\u00b6","text":"<p>A embedding API call has two required inputs:</p> <ul> <li><code>model</code>: the name of the model you want to use (e.g., <code>sentence-transformers/all-MiniLM-L6-v2</code>) or leave it empty to just call the api</li> <li><code>input</code>: a string or list of strings you want to embed</li> </ul> <p>Let's look at an example API calls to see how the chat format works in practice.</p>"},{"location":"examples/sagemaker-text-completion-api/","title":"How to use Text (Instruction) Completion clients","text":"In\u00a0[\u00a0]: Copied! <pre># if needed, install and/or upgrade to the latest version of the OpenAI Python library\n%pip install --upgrade easyllm\n</pre> # if needed, install and/or upgrade to the latest version of the OpenAI Python library %pip install --upgrade easyllm  In\u00a0[6]: Copied! <pre># import the EasyLLM Python library for calling the EasyLLM API\nimport easyllm\n</pre> # import the EasyLLM Python library for calling the EasyLLM API import easyllm In\u00a0[1]: Copied! <pre>import os \n# set env for prompt builder\nos.environ[\"HUGGINGFACE_PROMPT\"] = \"llama2\" # vicuna, wizardlm, stablebeluga, open_assistant\nos.environ[\"AWS_REGION\"] = \"us-east-1\"  # change to your region\n# os.environ[\"AWS_ACCESS_KEY_ID\"] = \"XXX\" # needed if not using boto3 session\n# os.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"XXX\" # needed if not using boto3 session\n\nfrom easyllm.clients import sagemaker\n\n# Changing configuration without using environment variables\n# sagemaker.prompt_builder = \"llama2\"\n# sagemaker.api_aws_access_key=\"xxx\"\n# sagemaker.api_aws_secret_key=\"xxx\"\n\n# SageMaker endpoint name\nMODEL=\"huggingface-pytorch-tgi-inference-2023-08-08-14-15-52-703\"\n\nresponse = sagemaker.Completion.create(\n    model=MODEL,\n    prompt=\"What is the meaning of life?\",\n    temperature=0.9,\n    top_p=0.6,\n    max_tokens=256,\n)\nresponse\n</pre> import os  # set env for prompt builder os.environ[\"HUGGINGFACE_PROMPT\"] = \"llama2\" # vicuna, wizardlm, stablebeluga, open_assistant os.environ[\"AWS_REGION\"] = \"us-east-1\"  # change to your region # os.environ[\"AWS_ACCESS_KEY_ID\"] = \"XXX\" # needed if not using boto3 session # os.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"XXX\" # needed if not using boto3 session  from easyllm.clients import sagemaker  # Changing configuration without using environment variables # sagemaker.prompt_builder = \"llama2\" # sagemaker.api_aws_access_key=\"xxx\" # sagemaker.api_aws_secret_key=\"xxx\"  # SageMaker endpoint name MODEL=\"huggingface-pytorch-tgi-inference-2023-08-08-14-15-52-703\"  response = sagemaker.Completion.create(     model=MODEL,     prompt=\"What is the meaning of life?\",     temperature=0.9,     top_p=0.6,     max_tokens=256, ) response Out[1]: <pre>{'id': 'hf-dEMeXTUk3Y',\n 'object': 'text.completion',\n 'created': 1691508711,\n 'model': 'huggingface-pytorch-tgi-inference-2023-08-08-14-15-52-703',\n 'choices': [{'index': 0,\n   'text': \" The meaning of life is a question that has puzzled philosophers, theologians, scientists, and many other thinkers throughout history. Here are some possible answers:\\n1. Religious or spiritual beliefs: Many people believe that the meaning of life is to fulfill a divine or spiritual purpose, whether that be to follow a set of moral commandments, to achieve spiritual enlightenment, or to fulfill a specific mission or calling.\\n2. Personal fulfillment: Some people believe that the meaning of life is to find personal fulfillment and happiness. This can be achieved through pursuing one's passions, building meaningful relationships, and cultivating a sense of purpose and meaning in one's life.\\n3. Contribution to society: Many people believe that the meaning of life is to make a positive impact on the world and to contribute to the greater good. This can be achieved through various means, such as working to make the world a better place, helping others, or creating something of value.\\n4. Learning and growth: Some people believe that the meaning of life is to learn and grow as individuals, to expand one's knowledge and understanding of the world, and to develop one's skills\",\n   'finish_reason': 'length'}],\n 'usage': {'prompt_tokens': 11, 'completion_tokens': 256, 'total_tokens': 267}}</pre> <p>As you can see, the response object has a few fields:</p> <ul> <li><code>id</code>: the ID of the request</li> <li><code>object</code>: the type of object returned (e.g., <code>text.completion</code>)</li> <li><code>created</code>: the timestamp of the request</li> <li><code>model</code>: the full name of the model used to generate the response</li> <li><code>usage</code>: the number of tokens used to generate the replies, counting prompt, completion, and total</li> <li><code>choices</code>: a list of completion objects (only one, unless you set <code>n</code> greater than 1)<ul> <li><code>text</code>: the generated text</li> <li><code>finish_reason</code>: the reason the model stopped generating text (either <code>stop</code>, <code>eos_token</code>, or <code>length</code> if <code>max_tokens</code> limit was reached)</li> <li><code>logprobs</code>: optional the log probs of each generated token.</li> </ul> </li> </ul> <p>Extract just the reply with:</p> In\u00a0[2]: Copied! <pre>print(response['choices'][0]['text'])\n</pre> print(response['choices'][0]['text']) <pre> The meaning of life is a question that has puzzled philosophers, theologians, scientists, and many other thinkers throughout history. Here are some possible answers:\n1. Religious or spiritual beliefs: Many people believe that the meaning of life is to fulfill a divine or spiritual purpose, whether that be to follow a set of moral commandments, to achieve spiritual enlightenment, or to fulfill a specific mission or calling.\n2. Personal fulfillment: Some people believe that the meaning of life is to find personal fulfillment and happiness. This can be achieved through pursuing one's passions, building meaningful relationships, and cultivating a sense of purpose and meaning in one's life.\n3. Contribution to society: Many people believe that the meaning of life is to make a positive impact on the world and to contribute to the greater good. This can be achieved through various means, such as working to make the world a better place, helping others, or creating something of value.\n4. Learning and growth: Some people believe that the meaning of life is to learn and grow as individuals, to expand one's knowledge and understanding of the world, and to develop one's skills\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"examples/sagemaker-text-completion-api/#how-to-use-text-instruction-completion-clients","title":"How to use Text (Instruction) Completion clients\u00b6","text":"<p>EasyLLM can be used as an abstract layer to replace <code>text-davinci-003</code> with open source models.</p> <p>You can change your own applications from the OpenAI API, by simply changing the client.</p> <p>Chat models take a series of messages as input, and return an AI-written message as output.</p> <p>This guide illustrates the chat format with a few example API calls.</p>"},{"location":"examples/sagemaker-text-completion-api/#0-setup","title":"0. Setup\u00b6","text":"<p>Before you can use <code>easyllm</code> with Amazon SageMaker you need to deploy the model to a SageMaker endpoint. You can do this by following one of the bloh posts below:</p> <ul> <li>Deploy Llama 2 7B/13B/70B on Amazon SageMaker</li> <li>Deploy Falcon 7B &amp; 40B on Amazon SageMaker</li> <li>Introducing the Hugging Face LLM Inference Container for Amazon SageMaker</li> </ul> <p>Once you have your endpoint deploy copy the endpoint name of it. The endpoint name will be our <code>model</code> paramter. You can get the endpoint name in the AWS management console for Amazon SageMaker under \"Inference\" -&gt; \"Endpoints\" -&gt; \"Name\" or when you deployed your model using the sagemaker sdk you can get it from the <code>predictor.endpoint_name</code> attribute.</p>"},{"location":"examples/sagemaker-text-completion-api/#1-import-the-easyllm-library","title":"1. Import the easyllm library\u00b6","text":""},{"location":"examples/sagemaker-text-completion-api/#2-an-example-chat-api-call","title":"2. An example chat API call\u00b6","text":"<p>A text API call has two required inputs:</p> <ul> <li><code>model</code>: the name of the model you want to use (e.g., <code>meta-llama/Llama-2-70b-chat-hf</code>) or leave it empty to just call the api</li> <li><code>prompt</code>: a text prompt that is sent to the model to generate the text</li> </ul> <p>Compared to OpenAI api is the <code>huggingface</code> module also exposing a <code>prompt_builder</code> and <code>stop_sequences</code> parameter you can use to customize the prompt and stop sequences. The EasyLLM package comes with build in popular methods for both of these parameters, e.g. <code>llama2_prompt_builder</code> and <code>llama2_stop_sequences</code>.</p> <p>Let's look at an example chat API calls to see how the chat format works in practice.</p>"},{"location":"examples/stream-chat-completions/","title":"How to stream Chat Completion requests","text":"In\u00a0[1]: Copied! <pre># imports\nimport easyllm  # for API calls\nimport time  # for measuring time duration of API calls\n</pre> # imports import easyllm  # for API calls import time  # for measuring time duration of API calls In\u00a0[3]: Copied! <pre>from easyllm.clients import huggingface\n\n# set the prompt builder to llama2\nhuggingface.prompt_builder = \"llama2\"\n\n# record the time before the request is sent\nstart_time = time.time()\n\n# send a ChatCompletion request to count to 100\nresponse = huggingface.ChatCompletion.create(\n    model=\"meta-llama/Llama-2-70b-chat-hf\",\n    messages=[\n        {'role': 'user', 'content': 'Count to 100, with a comma between each number and no newlines. E.g., 1, 2, 3, ...'}\n    ],\n)\n\n# calculate the time it took to receive the response\nresponse_time = time.time() - start_time\n\n# print the time delay and text received\nprint(f\"Full response received {response_time:.2f} seconds after request\")\nprint(f\"Full response received:\\n{response}\")\n</pre> from easyllm.clients import huggingface  # set the prompt builder to llama2 huggingface.prompt_builder = \"llama2\"  # record the time before the request is sent start_time = time.time()  # send a ChatCompletion request to count to 100 response = huggingface.ChatCompletion.create(     model=\"meta-llama/Llama-2-70b-chat-hf\",     messages=[         {'role': 'user', 'content': 'Count to 100, with a comma between each number and no newlines. E.g., 1, 2, 3, ...'}     ], )  # calculate the time it took to receive the response response_time = time.time() - start_time  # print the time delay and text received print(f\"Full response received {response_time:.2f} seconds after request\") print(f\"Full response received:\\n{response}\")  <pre>Full response received 0.12 seconds after request\nFull response received:\n{'id': 'hf-JhxbFCGVUW', 'object': 'chat.completion', 'created': 1691129826, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': ' Sure! Here it is:\\n\\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100'}, 'finish_reason': 'eos_token'}], 'usage': {'prompt_tokens': 25, 'completion_tokens': 400, 'total_tokens': 425}}\n</pre> <p>The reply can be extracted with <code>response['choices'][0]['message']</code>.</p> <p>The content of the reply can be extracted with <code>response['choices'][0]['message']['content']</code>.</p> In\u00a0[4]: Copied! <pre>reply = response['choices'][0]['message']\nprint(f\"Extracted reply: \\n{reply}\")\n\nreply_content = response['choices'][0]['message']['content']\nprint(f\"Extracted content: \\n{reply_content}\")\n</pre> reply = response['choices'][0]['message'] print(f\"Extracted reply: \\n{reply}\")  reply_content = response['choices'][0]['message']['content'] print(f\"Extracted content: \\n{reply_content}\")  <pre>Extracted reply: \n{'role': 'assistant', 'content': ' Sure! Here it is:\\n\\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100'}\nExtracted content: \n Sure! Here it is:\n\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100\n</pre> In\u00a0[5]: Copied! <pre>from easyllm.clients import huggingface\n\n\nhuggingface.prompt_builder = \"llama2\"\n\n# a ChatCompletion request\nresponse = huggingface.ChatCompletion.create(\n    model=\"meta-llama/Llama-2-70b-chat-hf\",\n    messages=[\n        {'role': 'user', 'content': \"What's 1+1? Answer in one word.\"}\n    ],\n    stream=True  # this time, we set stream=True\n)\n\nfor chunk in response:\n    print(chunk)\n</pre> from easyllm.clients import huggingface   huggingface.prompt_builder = \"llama2\"  # a ChatCompletion request response = huggingface.ChatCompletion.create(     model=\"meta-llama/Llama-2-70b-chat-hf\",     messages=[         {'role': 'user', 'content': \"What's 1+1? Answer in one word.\"}     ],     stream=True  # this time, we set stream=True )  for chunk in response:     print(chunk) <pre>{'id': 'hf--G5Fhg3YMu', 'object': 'chat.completion.chunk', 'created': 1691129830, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'delta': {'role': 'assistant'}}]}\n{'id': 'hf--G5Fhg3YMu', 'object': 'chat.completion.chunk', 'created': 1691129830, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'delta': {'content': ' '}}]}\n{'id': 'hf--G5Fhg3YMu', 'object': 'chat.completion.chunk', 'created': 1691129830, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'delta': {'content': ' Two'}}]}\n{'id': 'hf--G5Fhg3YMu', 'object': 'chat.completion.chunk', 'created': 1691129830, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'delta': {}}]}\n</pre> <p>As you can see above, streaming responses have a <code>delta</code> field rather than a <code>message</code> field. <code>delta</code> can hold things like:</p> <ul> <li>a role token (e.g., <code>{\"role\": \"assistant\"}</code>)</li> <li>a content token (e.g., <code>{\"content\": \"\\n\\n\"}</code>)</li> <li>nothing (e.g., <code>{}</code>), when the stream is over</li> </ul> In\u00a0[6]: Copied! <pre>import time\nfrom easyllm.clients import huggingface\n\nhuggingface.prompt_builder = \"llama2\"\n\n# record the time before the request is sent\nstart_time = time.time()\n\n# send a ChatCompletion request to count to 100\nresponse = huggingface.ChatCompletion.create(\n    model=\"meta-llama/Llama-2-70b-chat-hf\",\n    messages=[\n        {'role': 'user', 'content': 'Count to 50, with a comma between each number and no newlines. E.g., 1, 2, 3, ...'}\n    ],\n    stream=True  # again, we set stream=True\n)\n\n# create variables to collect the stream of chunks\ncollected_chunks = []\ncollected_messages = []\n# iterate through the stream of events\nfor chunk in response:\n    chunk_time = time.time() - start_time  # calculate the time delay of the chunk\n    collected_chunks.append(chunk)  # save the event response\n    chunk_message = chunk['choices'][0]['delta']  # extract the message\n    collected_messages.append(chunk_message)  # save the message\n    print(f\"Message received {chunk_time:.2f} seconds after request: {chunk_message}\")  # print the delay and text\n\n# print the time delay and text received\nprint(f\"Full response received {chunk_time:.2f} seconds after request\")\nfull_reply_content = ''.join([m.get('content', '') for m in collected_messages])\nprint(f\"Full conversation received: {full_reply_content}\")\n</pre> import time from easyllm.clients import huggingface  huggingface.prompt_builder = \"llama2\"  # record the time before the request is sent start_time = time.time()  # send a ChatCompletion request to count to 100 response = huggingface.ChatCompletion.create(     model=\"meta-llama/Llama-2-70b-chat-hf\",     messages=[         {'role': 'user', 'content': 'Count to 50, with a comma between each number and no newlines. E.g., 1, 2, 3, ...'}     ],     stream=True  # again, we set stream=True )  # create variables to collect the stream of chunks collected_chunks = [] collected_messages = [] # iterate through the stream of events for chunk in response:     chunk_time = time.time() - start_time  # calculate the time delay of the chunk     collected_chunks.append(chunk)  # save the event response     chunk_message = chunk['choices'][0]['delta']  # extract the message     collected_messages.append(chunk_message)  # save the message     print(f\"Message received {chunk_time:.2f} seconds after request: {chunk_message}\")  # print the delay and text  # print the time delay and text received print(f\"Full response received {chunk_time:.2f} seconds after request\") full_reply_content = ''.join([m.get('content', '') for m in collected_messages]) print(f\"Full conversation received: {full_reply_content}\")  <pre>Message received 0.13 seconds after request: {'role': 'assistant'}\nMessage received 0.13 seconds after request: {'content': ' '}\nMessage received 0.13 seconds after request: {'content': ' Sure'}\nMessage received 0.13 seconds after request: {'content': '!'}\nMessage received 0.13 seconds after request: {'content': ' Here'}\nMessage received 0.13 seconds after request: {'content': ' it'}\nMessage received 0.13 seconds after request: {'content': ' is'}\nMessage received 0.13 seconds after request: {'content': ':'}\nMessage received 0.13 seconds after request: {'content': '\\n'}\nMessage received 0.13 seconds after request: {'content': '\\n'}\nMessage received 0.13 seconds after request: {'content': '1'}\nMessage received 0.13 seconds after request: {'content': ','}\nMessage received 0.13 seconds after request: {'content': ' '}\nMessage received 0.13 seconds after request: {'content': '2'}\nMessage received 0.13 seconds after request: {'content': ','}\nMessage received 0.13 seconds after request: {'content': ' '}\nMessage received 0.13 seconds after request: {'content': '3'}\nMessage received 0.13 seconds after request: {'content': ','}\nMessage received 0.13 seconds after request: {'content': ' '}\nMessage received 0.13 seconds after request: {'content': '4'}\nMessage received 0.13 seconds after request: {'content': ','}\nMessage received 0.13 seconds after request: {'content': ' '}\nMessage received 0.13 seconds after request: {'content': '5'}\nMessage received 0.13 seconds after request: {'content': ','}\nMessage received 0.13 seconds after request: {'content': ' '}\nMessage received 0.13 seconds after request: {'content': '6'}\nMessage received 0.13 seconds after request: {'content': ','}\nMessage received 0.13 seconds after request: {'content': ' '}\nMessage received 0.13 seconds after request: {'content': '7'}\nMessage received 0.13 seconds after request: {'content': ','}\nMessage received 0.13 seconds after request: {'content': ' '}\nMessage received 0.13 seconds after request: {'content': '8'}\nMessage received 0.13 seconds after request: {'content': ','}\nMessage received 0.13 seconds after request: {'content': ' '}\nMessage received 0.13 seconds after request: {'content': '9'}\nMessage received 0.13 seconds after request: {'content': ','}\nMessage received 0.13 seconds after request: {'content': ' '}\nMessage received 0.13 seconds after request: {'content': '1'}\nMessage received 0.13 seconds after request: {'content': '0'}\nMessage received 0.13 seconds after request: {'content': ','}\nMessage received 0.13 seconds after request: {'content': ' '}\nMessage received 0.13 seconds after request: {'content': '1'}\nMessage received 0.13 seconds after request: {'content': '1'}\nMessage received 0.13 seconds after request: {'content': ','}\nMessage received 0.13 seconds after request: {'content': ' '}\nMessage received 0.13 seconds after request: {'content': '1'}\nMessage received 0.13 seconds after request: {'content': '2'}\nMessage received 0.13 seconds after request: {'content': ','}\nMessage received 0.13 seconds after request: {'content': ' '}\nMessage received 0.13 seconds after request: {'content': '1'}\nMessage received 0.13 seconds after request: {'content': '3'}\nMessage received 0.13 seconds after request: {'content': ','}\nMessage received 0.13 seconds after request: {'content': ' '}\nMessage received 0.13 seconds after request: {'content': '1'}\nMessage received 0.13 seconds after request: {'content': '4'}\nMessage received 0.13 seconds after request: {'content': ','}\nMessage received 0.13 seconds after request: {'content': ' '}\nMessage received 0.13 seconds after request: {'content': '1'}\nMessage received 0.13 seconds after request: {'content': '5'}\nMessage received 0.13 seconds after request: {'content': ','}\nMessage received 0.13 seconds after request: {'content': ' '}\nMessage received 0.13 seconds after request: {'content': '1'}\nMessage received 0.13 seconds after request: {'content': '6'}\nMessage received 0.13 seconds after request: {'content': ','}\nMessage received 0.13 seconds after request: {'content': ' '}\nMessage received 0.13 seconds after request: {'content': '1'}\nMessage received 0.13 seconds after request: {'content': '7'}\nMessage received 0.13 seconds after request: {'content': ','}\nMessage received 0.13 seconds after request: {'content': ' '}\nMessage received 0.13 seconds after request: {'content': '1'}\nMessage received 0.13 seconds after request: {'content': '8'}\nMessage received 0.13 seconds after request: {'content': ','}\nMessage received 0.13 seconds after request: {'content': ' '}\nMessage received 0.13 seconds after request: {'content': '1'}\nMessage received 0.13 seconds after request: {'content': '9'}\nMessage received 0.13 seconds after request: {'content': ','}\nMessage received 0.13 seconds after request: {'content': ' '}\nMessage received 0.13 seconds after request: {'content': '2'}\nMessage received 0.13 seconds after request: {'content': '0'}\nMessage received 0.13 seconds after request: {'content': ','}\nMessage received 0.13 seconds after request: {'content': ' '}\nMessage received 0.13 seconds after request: {'content': '2'}\nMessage received 0.13 seconds after request: {'content': '1'}\nMessage received 0.13 seconds after request: {'content': ','}\nMessage received 0.13 seconds after request: {'content': ' '}\nMessage received 0.13 seconds after request: {'content': '2'}\nMessage received 0.13 seconds after request: {'content': '2'}\nMessage received 0.13 seconds after request: {'content': ','}\nMessage received 0.13 seconds after request: {'content': ' '}\nMessage received 0.13 seconds after request: {'content': '2'}\nMessage received 0.13 seconds after request: {'content': '3'}\nMessage received 0.13 seconds after request: {'content': ','}\nMessage received 0.13 seconds after request: {'content': ' '}\nMessage received 0.13 seconds after request: {'content': '2'}\nMessage received 0.13 seconds after request: {'content': '4'}\nMessage received 0.13 seconds after request: {'content': ','}\nMessage received 0.13 seconds after request: {'content': ' '}\nMessage received 0.13 seconds after request: {'content': '2'}\nMessage received 0.13 seconds after request: {'content': '5'}\nMessage received 0.13 seconds after request: {'content': ','}\nMessage received 0.13 seconds after request: {'content': ' '}\nMessage received 0.13 seconds after request: {'content': '2'}\nMessage received 0.13 seconds after request: {'content': '6'}\nMessage received 0.13 seconds after request: {'content': ','}\nMessage received 0.13 seconds after request: {'content': ' '}\nMessage received 0.13 seconds after request: {'content': '2'}\nMessage received 0.13 seconds after request: {'content': '7'}\nMessage received 0.13 seconds after request: {'content': ','}\nMessage received 0.13 seconds after request: {'content': ' '}\nMessage received 0.13 seconds after request: {'content': '2'}\nMessage received 0.13 seconds after request: {'content': '8'}\nMessage received 0.13 seconds after request: {'content': ','}\nMessage received 0.13 seconds after request: {'content': ' '}\nMessage received 0.13 seconds after request: {'content': '2'}\nMessage received 0.13 seconds after request: {'content': '9'}\nMessage received 0.13 seconds after request: {'content': ','}\nMessage received 0.13 seconds after request: {'content': ' '}\nMessage received 0.13 seconds after request: {'content': '3'}\nMessage received 0.13 seconds after request: {'content': '0'}\nMessage received 0.13 seconds after request: {'content': ','}\nMessage received 0.13 seconds after request: {'content': ' '}\nMessage received 0.13 seconds after request: {'content': '3'}\nMessage received 0.13 seconds after request: {'content': '1'}\nMessage received 0.13 seconds after request: {'content': ','}\nMessage received 0.13 seconds after request: {'content': ' '}\nMessage received 0.13 seconds after request: {'content': '3'}\nMessage received 0.13 seconds after request: {'content': '2'}\nMessage received 0.13 seconds after request: {'content': ','}\nMessage received 0.13 seconds after request: {'content': ' '}\nMessage received 0.13 seconds after request: {'content': '3'}\nMessage received 0.13 seconds after request: {'content': '3'}\nMessage received 0.13 seconds after request: {'content': ','}\nMessage received 0.13 seconds after request: {'content': ' '}\nMessage received 0.13 seconds after request: {'content': '3'}\nMessage received 0.13 seconds after request: {'content': '4'}\nMessage received 0.13 seconds after request: {'content': ','}\nMessage received 0.13 seconds after request: {'content': ' '}\nMessage received 0.13 seconds after request: {'content': '3'}\nMessage received 0.13 seconds after request: {'content': '5'}\nMessage received 0.13 seconds after request: {'content': ','}\nMessage received 0.13 seconds after request: {'content': ' '}\nMessage received 0.13 seconds after request: {'content': '3'}\nMessage received 0.13 seconds after request: {'content': '6'}\nMessage received 0.13 seconds after request: {'content': ','}\nMessage received 0.13 seconds after request: {'content': ' '}\nMessage received 0.13 seconds after request: {'content': '3'}\nMessage received 0.13 seconds after request: {'content': '7'}\nMessage received 0.13 seconds after request: {'content': ','}\nMessage received 0.13 seconds after request: {'content': ' '}\nMessage received 0.13 seconds after request: {'content': '3'}\nMessage received 0.13 seconds after request: {'content': '8'}\nMessage received 0.13 seconds after request: {'content': ','}\nMessage received 0.13 seconds after request: {'content': ' '}\nMessage received 0.13 seconds after request: {'content': '3'}\nMessage received 0.13 seconds after request: {'content': '9'}\nMessage received 0.13 seconds after request: {'content': ','}\nMessage received 0.13 seconds after request: {'content': ' '}\nMessage received 0.13 seconds after request: {'content': '4'}\nMessage received 0.13 seconds after request: {'content': '0'}\nMessage received 0.13 seconds after request: {'content': ','}\nMessage received 0.13 seconds after request: {'content': ' '}\nMessage received 0.13 seconds after request: {'content': '4'}\nMessage received 0.13 seconds after request: {'content': '1'}\nMessage received 0.13 seconds after request: {'content': ','}\nMessage received 0.13 seconds after request: {'content': ' '}\nMessage received 0.13 seconds after request: {'content': '4'}\nMessage received 0.13 seconds after request: {'content': '2'}\nMessage received 0.13 seconds after request: {'content': ','}\nMessage received 0.13 seconds after request: {'content': ' '}\nMessage received 0.13 seconds after request: {'content': '4'}\nMessage received 0.13 seconds after request: {'content': '3'}\nMessage received 0.13 seconds after request: {'content': ','}\nMessage received 0.13 seconds after request: {'content': ' '}\nMessage received 0.13 seconds after request: {'content': '4'}\nMessage received 0.13 seconds after request: {'content': '4'}\nMessage received 0.13 seconds after request: {'content': ','}\nMessage received 0.13 seconds after request: {'content': ' '}\nMessage received 0.13 seconds after request: {'content': '4'}\nMessage received 0.13 seconds after request: {'content': '5'}\nMessage received 0.13 seconds after request: {'content': ','}\nMessage received 0.13 seconds after request: {'content': ' '}\nMessage received 0.13 seconds after request: {'content': '4'}\nMessage received 0.13 seconds after request: {'content': '6'}\nMessage received 0.13 seconds after request: {'content': ','}\nMessage received 0.13 seconds after request: {'content': ' '}\nMessage received 0.13 seconds after request: {'content': '4'}\nMessage received 0.13 seconds after request: {'content': '7'}\nMessage received 0.13 seconds after request: {'content': ','}\nMessage received 0.13 seconds after request: {'content': ' '}\nMessage received 0.13 seconds after request: {'content': '4'}\nMessage received 0.13 seconds after request: {'content': '8'}\nMessage received 0.13 seconds after request: {'content': ','}\nMessage received 0.13 seconds after request: {'content': ' '}\nMessage received 0.13 seconds after request: {'content': '4'}\nMessage received 0.13 seconds after request: {'content': '9'}\nMessage received 0.13 seconds after request: {'content': ','}\nMessage received 0.13 seconds after request: {'content': ' '}\nMessage received 0.13 seconds after request: {'content': '5'}\nMessage received 0.13 seconds after request: {'content': '0'}\nMessage received 0.13 seconds after request: {}\nFull response received 0.13 seconds after request\nFull conversation received:   Sure! Here it is:\n\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50\n</pre>"},{"location":"examples/stream-chat-completions/#how-to-stream-chat-completion-requests","title":"How to stream Chat Completion requests\u00b6","text":"<p>By default, when you request a completion, the entire completion is generated before being sent back in a single response.</p> <p>If you're generating long completions, waiting for the response can take many seconds.</p> <p>To get responses sooner, you can 'stream' the completion as it's being generated. This allows you to start printing or processing the beginning of the completion before the full completion is finished.</p> <p>To stream completions, set <code>stream=True</code> when calling the chat completions or completions endpoints. This will return an object that streams back the response as data-only server-sent events. Extract chunks from the <code>delta</code> field rather than the <code>message</code> field.</p>"},{"location":"examples/stream-chat-completions/#downsides","title":"Downsides\u00b6","text":"<p>Note that using <code>stream=True</code> in a production application makes it more difficult to moderate the content of the completions, as partial completions may be more difficult to evaluate.</p>"},{"location":"examples/stream-chat-completions/#example-code","title":"Example code\u00b6","text":"<p>Below, this notebook shows:</p> <ol> <li>What a typical chat completion response looks like</li> <li>What a streaming chat completion response looks like</li> <li>How much time is saved by streaming a chat completion</li> </ol>"},{"location":"examples/stream-chat-completions/#1-what-a-typical-chat-completion-response-looks-like","title":"1. What a typical chat completion response looks like\u00b6","text":"<p>With a typical ChatCompletions API call, the response is first computed and then returned all at once.</p>"},{"location":"examples/stream-chat-completions/#2-how-to-stream-a-chat-completion","title":"2. How to stream a chat completion\u00b6","text":"<p>With a streaming API call, the response is sent back incrementally in chunks via an event stream. In Python, you can iterate over these events with a <code>for</code> loop.</p> <p>Let's see what it looks like:</p>"},{"location":"examples/stream-chat-completions/#3-how-much-time-is-saved-by-streaming-a-chat-completion","title":"3. How much time is saved by streaming a chat completion\u00b6","text":"<p>Now let's ask <code>meta-llama/Llama-2-70b-chat-hf</code> to count to 100 again, and see how long it takes.</p>"},{"location":"examples/stream-chat-completions/#time-comparison","title":"Time comparison\u00b6","text":"<p>In the example above, both requests took about 3 seconds to fully complete. Request times will vary depending on load and other stochastic factors.</p> <p>However, with the streaming request, we received the first token after 0.1 seconds, and subsequent tokens every ~0.01-0.02 seconds.</p>"},{"location":"examples/stream-text-completions/","title":"How to stream Text Completion requests","text":"In\u00a0[1]: Copied! <pre># imports\nimport easyllm  # for API calls\nimport time  # for measuring time duration of API calls\n</pre> # imports import easyllm  # for API calls import time  # for measuring time duration of API calls In\u00a0[1]: Copied! <pre>from easyllm.clients import huggingface\n\nhuggingface.prompt_builder = \"llama2\"\n\n# a ChatCompletion request\nresponse = huggingface.Completion.create(\n    model=\"meta-llama/Llama-2-70b-chat-hf\",\n    prompt=\"Count to 50, with a comma between each number and no newlines. E.g., 1, 2, 3, ...\",\n    stream=True  # this time, we set stream=True\n)\n\nfor chunk in response:\n    print(chunk)\n</pre> from easyllm.clients import huggingface  huggingface.prompt_builder = \"llama2\"  # a ChatCompletion request response = huggingface.Completion.create(     model=\"meta-llama/Llama-2-70b-chat-hf\",     prompt=\"Count to 50, with a comma between each number and no newlines. E.g., 1, 2, 3, ...\",     stream=True  # this time, we set stream=True )  for chunk in response:     print(chunk) <pre>{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ' ', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ' Sure', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '!', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ' Here', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ' it', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ' is', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ':', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '\\n', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '\\n', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '1', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ',', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ' ', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '2', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ',', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ' ', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '3', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ',', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ' ', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '4', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ',', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ' ', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '5', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ',', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ' ', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '6', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ',', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ' ', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '7', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ',', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ' ', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '8', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ',', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ' ', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '9', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ',', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ' ', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '1', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '0', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ',', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ' ', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '1', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '1', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ',', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ' ', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '1', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '2', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ',', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ' ', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '1', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '3', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ',', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ' ', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '1', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '4', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ',', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ' ', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '1', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '5', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ',', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ' ', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '1', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '6', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ',', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ' ', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '1', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '7', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ',', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ' ', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '1', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '8', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ',', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ' ', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '1', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '9', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ',', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ' ', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '2', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '0', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ',', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ' ', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '2', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '1', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ',', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ' ', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '2', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '2', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ',', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ' ', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '2', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '3', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ',', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ' ', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '2', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '4', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ',', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ' ', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '2', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '5', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ',', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ' ', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '2', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '6', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ',', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ' ', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '2', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '7', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ',', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ' ', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '2', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '8', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ',', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ' ', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '2', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '9', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ',', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ' ', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '3', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '0', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ',', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ' ', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '3', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '1', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ',', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ' ', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '3', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '2', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ',', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ' ', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '3', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '3', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ',', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ' ', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '3', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '4', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ',', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ' ', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '3', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '5', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ',', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ' ', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '3', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '6', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ',', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ' ', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '3', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '7', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ',', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ' ', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '3', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '8', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ',', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ' ', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '3', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '9', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ',', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ' ', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '4', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '0', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ',', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ' ', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '4', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '1', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ',', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ' ', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '4', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '2', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ',', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ' ', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '4', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '3', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ',', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ' ', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '4', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '4', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ',', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ' ', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '4', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '5', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ',', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ' ', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '4', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '6', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ',', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ' ', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '4', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '7', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ',', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ' ', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '4', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '8', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ',', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ' ', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '4', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '9', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ',', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ' ', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '5', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '0', 'logprobs': 0.0}]}\n</pre> <p>As you can see above, streaming responses have a <code>text</code> field which holds the generated tokens.</p> <p>Below is an example where we print the generated text as it comes in, and stop when we see a <code>&lt;/s&gt;</code> token.</p> In\u00a0[2]: Copied! <pre>from easyllm.clients import huggingface\n\nhuggingface.prompt_builder = \"llama2\"\n\n# a ChatCompletion request\nresponse = huggingface.Completion.create(\n    model=\"meta-llama/Llama-2-70b-chat-hf\",\n    prompt=\"Count to 10, with a comma between each number and no newlines. E.g., 1, 2, 3, ...\",\n    stream=True  # this time, we set stream=True\n)\n\nfor chunk in response:\n    print(chunk[\"choices\"][0][\"text\"],end=\"\")\n</pre> from easyllm.clients import huggingface  huggingface.prompt_builder = \"llama2\"  # a ChatCompletion request response = huggingface.Completion.create(     model=\"meta-llama/Llama-2-70b-chat-hf\",     prompt=\"Count to 10, with a comma between each number and no newlines. E.g., 1, 2, 3, ...\",     stream=True  # this time, we set stream=True )  for chunk in response:     print(chunk[\"choices\"][0][\"text\"],end=\"\") <pre>  Sure! Here it is: 1, 2, 3, 4, 5, 6, 7, 8, 9, 10.</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"examples/stream-text-completions/#how-to-stream-text-completion-requests","title":"How to stream Text Completion requests\u00b6","text":"<p>By default, when you request a completion, the entire completion is generated before being sent back in a single response.</p> <p>If you're generating long completions, waiting for the response can take many seconds.</p> <p>To get responses sooner, you can 'stream' the completion as it's being generated. This allows you to start printing or processing the beginning of the completion before the full completion is finished.</p> <p>To stream completions, set <code>stream=True</code> when calling the chat completions or completions endpoints. This will return an object that streams back the response as data-only server-sent events. Extract chunks from the <code>delta</code> field rather than the <code>message</code> field.</p>"},{"location":"examples/stream-text-completions/#downsides","title":"Downsides\u00b6","text":"<p>Note that using <code>stream=True</code> in a production application makes it more difficult to moderate the content of the completions, as partial completions may be more difficult to evaluate.</p>"},{"location":"examples/stream-text-completions/#example-code","title":"Example code\u00b6","text":"<p>Below, this notebook shows:</p> <ol> <li>What a streaming text completion response looks like</li> </ol>"},{"location":"examples/stream-text-completions/#2-how-to-stream-a-text-completion","title":"2. How to stream a text completion\u00b6","text":"<p>With a streaming API call, the response is sent back incrementally in chunks via an event stream. In Python, you can iterate over these events with a <code>for</code> loop.</p> <p>Let's see what it looks like:</p>"},{"location":"examples/text-completion-api/","title":"How to use Text (Instruction) Completion clients","text":"In\u00a0[\u00a0]: Copied! <pre># if needed, install and/or upgrade to the latest version of the OpenAI Python library\n%pip install --upgrade easyllm\n</pre> # if needed, install and/or upgrade to the latest version of the OpenAI Python library %pip install --upgrade easyllm  In\u00a0[6]: Copied! <pre># import the EasyLLM Python library for calling the EasyLLM API\nimport easyllm\n</pre> # import the EasyLLM Python library for calling the EasyLLM API import easyllm In\u00a0[1]: Copied! <pre>from easyllm.clients import huggingface\n\n# Example EasyLLM Python library request\nMODEL = \"meta-llama/Llama-2-70b-chat-hf\"\nhuggingface.prompt_builder = \"llama2\"\n\n# The module automatically loads the HuggingFace API key from the environment variable HUGGINGFACE_TOKEN or from the HuggingFace CLI configuration file.\n# huggingface.api_key=\"hf_xxx\"\n\nresponse = huggingface.Completion.create(\n    model=MODEL,\n    prompt=\"What is the meaning of life?\",\n    temperature=0.9,\n    top_p=0.6,\n    max_tokens=1024,\n)\nresponse\n</pre>  from easyllm.clients import huggingface  # Example EasyLLM Python library request MODEL = \"meta-llama/Llama-2-70b-chat-hf\" huggingface.prompt_builder = \"llama2\"  # The module automatically loads the HuggingFace API key from the environment variable HUGGINGFACE_TOKEN or from the HuggingFace CLI configuration file. # huggingface.api_key=\"hf_xxx\"  response = huggingface.Completion.create(     model=MODEL,     prompt=\"What is the meaning of life?\",     temperature=0.9,     top_p=0.6,     max_tokens=1024, ) response Out[1]: <pre>{'id': 'hf-ZK--Ndk30h',\n 'object': 'text.completion',\n 'created': 1691129933,\n 'model': 'meta-llama/Llama-2-70b-chat-hf',\n 'choices': [{'index': 0,\n   'text': \" The meaning of life is a question that has puzzled philosophers, theologians, and scientists for centuries. There are many different perspectives on what constitutes the meaning of life, and there is no one definitive answer. However, some common themes that people often associate with the meaning of life include:\\n\\n1. Purpose: Having a sense of purpose or direction in life, whether it be through work, relationships, or personal goals.\\n2. Fulfillment: Feeling fulfilled and satisfied with one's experiences and achievements.\\n3. Happiness: Pursuing happiness and well-being, whether through personal relationships, material possessions, or personal growth.\\n4. Self-actualization: Realizing one's potential and living up to one's capabilities.\\n5. Legacy: Leaving a lasting impact or legacy, whether through contributions to society, artistic or cultural achievements, or impacting the lives of others.\\n6. Spirituality: Connecting with a higher power or a sense of something greater than oneself, and finding meaning and purpose through faith or spiritual practices.\\n7. Love: Finding and experiencing love, whether it be through romantic relationships, friendships, or family.\\n8. Personal growth: Continuously learning, growing, and improving oneself.\\n9. Community: Building and being a part of a community, whether it be through work, volunteering, or social connections.\\n10. Making a difference: Making a positive impact in the world and leaving it a better place than when you arrived.\\n\\nUltimately, the meaning of life is a deeply personal and subjective question, and what gives meaning and purpose to one person's life may be different for another. It's a question that each person must answer for themselves, and it may change throughout their life as they grow and evolve.\",\n   'finish_reason': 'eos_token'}],\n 'usage': {'prompt_tokens': 11, 'completion_tokens': 406, 'total_tokens': 417}}</pre> <p>As you can see, the response object has a few fields:</p> <ul> <li><code>id</code>: the ID of the request</li> <li><code>object</code>: the type of object returned (e.g., <code>text.completion</code>)</li> <li><code>created</code>: the timestamp of the request</li> <li><code>model</code>: the full name of the model used to generate the response</li> <li><code>usage</code>: the number of tokens used to generate the replies, counting prompt, completion, and total</li> <li><code>choices</code>: a list of completion objects (only one, unless you set <code>n</code> greater than 1)<ul> <li><code>text</code>: the generated text</li> <li><code>finish_reason</code>: the reason the model stopped generating text (either <code>stop</code>, <code>eos_token</code>, or <code>length</code> if <code>max_tokens</code> limit was reached)</li> <li><code>logprobs</code>: optional the log probs of each generated token.</li> </ul> </li> </ul> <p>Extract just the reply with:</p> In\u00a0[2]: Copied! <pre>print(response['choices'][0]['text'])\n</pre> print(response['choices'][0]['text']) <pre> The meaning of life is a question that has puzzled philosophers, theologians, and scientists for centuries. There are many different perspectives on what constitutes the meaning of life, and there is no one definitive answer. However, some common themes that people often associate with the meaning of life include:\n\n1. Purpose: Having a sense of purpose or direction in life, whether it be through work, relationships, or personal goals.\n2. Fulfillment: Feeling fulfilled and satisfied with one's experiences and achievements.\n3. Happiness: Pursuing happiness and well-being, whether through personal relationships, material possessions, or personal growth.\n4. Self-actualization: Realizing one's potential and living up to one's capabilities.\n5. Legacy: Leaving a lasting impact or legacy, whether through contributions to society, artistic or cultural achievements, or impacting the lives of others.\n6. Spirituality: Connecting with a higher power or a sense of something greater than oneself, and finding meaning and purpose through faith or spiritual practices.\n7. Love: Finding and experiencing love, whether it be through romantic relationships, friendships, or family.\n8. Personal growth: Continuously learning, growing, and improving oneself.\n9. Community: Building and being a part of a community, whether it be through work, volunteering, or social connections.\n10. Making a difference: Making a positive impact in the world and leaving it a better place than when you arrived.\n\nUltimately, the meaning of life is a deeply personal and subjective question, and what gives meaning and purpose to one person's life may be different for another. It's a question that each person must answer for themselves, and it may change throughout their life as they grow and evolve.\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"examples/text-completion-api/#how-to-use-text-instruction-completion-clients","title":"How to use Text (Instruction) Completion clients\u00b6","text":"<p>EasyLLM can be used as an abstract layer to replace <code>text-davinci-003</code> with open source models.</p> <p>You can change your own applications from the OpenAI API, by simply changing the client.</p> <p>Chat models take a series of messages as input, and return an AI-written message as output.</p> <p>This guide illustrates the chat format with a few example API calls.</p>"},{"location":"examples/text-completion-api/#1-import-the-easyllm-library","title":"1. Import the easyllm library\u00b6","text":""},{"location":"examples/text-completion-api/#2-an-example-chat-api-call","title":"2. An example chat API call\u00b6","text":"<p>A text API call has two required inputs:</p> <ul> <li><code>model</code>: the name of the model you want to use (e.g., <code>meta-llama/Llama-2-70b-chat-hf</code>) or leave it empty to just call the api</li> <li><code>prompt</code>: a text prompt that is sent to the model to generate the text</li> </ul> <p>Compared to OpenAI api is the <code>huggingface</code> module also exposing a <code>prompt_builder</code> and <code>stop_sequences</code> parameter you can use to customize the prompt and stop sequences. The EasyLLM package comes with build in popular methods for both of these parameters, e.g. <code>llama2_prompt_builder</code> and <code>llama2_stop_sequences</code>.</p> <p>Let's look at an example chat API calls to see how the chat format works in practice.</p>"}]}